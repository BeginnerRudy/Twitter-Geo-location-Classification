{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-19T12:58:49.516459Z",
     "start_time": "2019-05-19T12:58:40.401836Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\software\\anaconda\\lib\\site-packages\\smart_open\\ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n",
      "C:\\software\\anaconda\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\91260\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\91260\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import time\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords  \n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read raw text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-19T12:58:49.550368Z",
     "start_time": "2019-05-19T12:58:49.546381Z"
    }
   },
   "outputs": [],
   "source": [
    "train_raw_filepath =  \"./2019S1-proj2-data_dos/train-raw.tsv\"\n",
    "dev_raw_filepath =  \"./2019S1-proj2-data_dos/dev-raw.tsv\"\n",
    "test_raw_filepath =  \"./2019S1-proj2-data_dos/test-raw.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-19T12:58:49.564362Z",
     "start_time": "2019-05-19T12:58:49.552365Z"
    }
   },
   "outputs": [],
   "source": [
    "# read tsv file into a 2D array\n",
    "def read_tsv(filepath):\n",
    "    label, text = [], []\n",
    "    with open(filepath) as raw:\n",
    "        reader = csv.reader(raw, delimiter=\"\\t\", quoting = csv.QUOTE_NONE)\n",
    "        for row in reader:\n",
    "            label.append(row[1])\n",
    "            text.append(row[-1])\n",
    "    return text, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-19T12:58:49.578324Z",
     "start_time": "2019-05-19T12:58:49.566325Z"
    }
   },
   "outputs": [],
   "source": [
    "def is_all_zero(instance):\n",
    "    for attr in instance:\n",
    "        if float(attr) != 0.0:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def remove_all_zero_instance(X, y):\n",
    "    new_X, new_y = [], []\n",
    "    for i in range(len(y)):\n",
    "        instance = X[i]\n",
    "        if is_all_zero(instance) is False:\n",
    "            new_X.append(X[i])\n",
    "            new_y.append(y[i])\n",
    "    return new_X, new_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Pre-filtering  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-19T12:58:49.595248Z",
     "start_time": "2019-05-19T12:58:49.580289Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_URL(texts):\n",
    "    for i in list(range(len(texts))):\n",
    "        texts[i] = re.sub(r\"http\\S+\", \"\", texts[i])\n",
    "        \n",
    "def remove_metion(texts):\n",
    "    for i in list(range(len(texts))):\n",
    "        texts[i] = re.sub(r\"@\\S+\", \"\", texts[i])\n",
    "        texts[i] = re.sub(r\"@\", \"\", texts[i])\n",
    "        \n",
    "def remove_hash(texts):\n",
    "    for i in list(range(len(texts))):\n",
    "        texts[i] = re.sub(r\"#\\S+\", \"\", texts[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WHL calculating "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-19T12:58:49.615195Z",
     "start_time": "2019-05-19T12:58:49.597244Z"
    }
   },
   "outputs": [],
   "source": [
    "class Word_Counter:\n",
    "    # Initializer / Instance Attributes\n",
    "    def __init__(self, size):\n",
    "        self.total_word_count = [0]*size\n",
    "        self.Melbourne_word_count = [0]*size\n",
    "        self.Sydney_word_count = [0]*size \n",
    "        self.Perth_word_count = [0]*size        \n",
    "        self.Brisbane_word_count = [0]*size        \n",
    "class WLH_lists:\n",
    "    # Initializer / Instance Attributes\n",
    "    def __init__(self):\n",
    "        self.Melbourne_WLH_list = []\n",
    "        self.Sydney_WLH_list = [] \n",
    "        self.Perth_WLH_list = []        \n",
    "        self.Brisbane_WLH_list = []        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-19T12:58:49.630154Z",
     "start_time": "2019-05-19T12:58:49.617191Z"
    }
   },
   "outputs": [],
   "source": [
    "def word_frequency_counter(counts, lables):\n",
    "    # create a list to store count\n",
    "    num_words = counts.shape[1]\n",
    "    counter = Word_Counter(num_words)\n",
    "    \n",
    "    for i in list(range(len(lables))):\n",
    "        instance = counts[i, :]\n",
    "        if (lables[i] == \"Melbourne\"):\n",
    "            counter.Melbourne_word_count += instance.toarray().sum(axis=0)\n",
    "        elif (lables[i] == \"Sydney\"):\n",
    "            counter.Sydney_word_count  += instance.toarray().sum(axis=0)\n",
    "        elif (lables[i] == \"Perth\"):\n",
    "            counter.Perth_word_count += instance.toarray().sum(axis=0)\n",
    "        elif (lables[i] == \"Brisbane\"):\n",
    "            counter.Brisbane_word_count += instance.toarray().sum(axis=0)\n",
    "    \n",
    "    counter.total_word_count = counter.Melbourne_word_count + \\\n",
    "                                counter.Sydney_word_count +\\\n",
    "                                counter.Perth_word_count +\\\n",
    "                                counter.Brisbane_word_count\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-19T12:58:49.646112Z",
     "start_time": "2019-05-19T12:58:49.632150Z"
    }
   },
   "outputs": [],
   "source": [
    "def occur_in_n_state(WHL_list):\n",
    "    count = 0\n",
    "    for WHL in WHL_list:\n",
    "        if WHL > 0:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-19T12:59:36.540838Z",
     "start_time": "2019-05-19T12:59:36.524882Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_WHL_by_state(word_counter, vectoriser):\n",
    "    WLH_list_occur_more_than_one_state = []\n",
    "    WLH_list_one_state = []\n",
    "    \n",
    "    #  total number of word used for each word counter\n",
    "    Melbourne_total_word_num = word_counter.Melbourne_word_count.sum()\n",
    "    Brisbane_total_word_num = word_counter.Brisbane_word_count.sum()\n",
    "    Sydney_total_word_num = word_counter.Sydney_word_count.sum()\n",
    "    Perth_total_word_num = word_counter.Perth_word_count.sum()\n",
    "    all_state_total_word_num = word_counter.total_word_count.sum()\n",
    "\n",
    "    for i in list(range(len(word_counter.total_word_count))):    \n",
    "    # for i in list(range(100)):\n",
    "        curr_word_list = [] # [WHL, word, frequency,  state]\n",
    "        curr_word = vectoriser.get_feature_names()[i]\n",
    "\n",
    "        curr_word_total_prob = word_counter.total_word_count[i]/all_state_total_word_num\n",
    "\n",
    "        WHL_Mel = (word_counter.Melbourne_word_count[i]/Melbourne_total_word_num)/curr_word_total_prob\n",
    "        WHL_Syd = (word_counter.Sydney_word_count[i]/Sydney_total_word_num)/curr_word_total_prob\n",
    "        WHL_Per = (word_counter.Perth_word_count[i]/Perth_total_word_num)/curr_word_total_prob\n",
    "        WHL_Bri = (word_counter.Brisbane_word_count[i]/Brisbane_total_word_num)/curr_word_total_prob\n",
    "\n",
    "        WHL_list = [WHL_Mel, WHL_Bri, WHL_Per, WHL_Syd]\n",
    "        if (occur_in_n_state(WHL_list) > 0):\n",
    "            max_WHL = max(WHL_list)\n",
    "\n",
    "            state = [] # (stateName, wordStateCount, wordTotalCount)\n",
    "            if (WHL_Mel == max_WHL):\n",
    "                state.append([\"Melbourne\", word_counter.Melbourne_word_count[i], word_counter.Melbourne_word_count[i]/word_counter.total_word_count[i]])\n",
    "            if (WHL_Bri == max_WHL):\n",
    "                state.append([\"Brisbane\", word_counter.Brisbane_word_count[i], word_counter.Brisbane_word_count[i]/word_counter.total_word_count[i]])\n",
    "            if (WHL_Per == max_WHL):\n",
    "                state.append([\"Perth\", word_counter.Perth_word_count[i], word_counter.Perth_word_count[i]/word_counter.total_word_count[i]])\n",
    "            if (WHL_Syd == max_WHL):\n",
    "                state.append([\"Sydney\", word_counter.Sydney_word_count[i], word_counter.Sydney_word_count[i]/word_counter.total_word_count[i]])\n",
    "            curr_word_list.append(max_WHL)\n",
    "            curr_word_list.append(i)\n",
    "            curr_word_list.append(curr_word)\n",
    "            curr_word_list.append(state)\n",
    "            WLH_list_occur_more_than_one_state.append(curr_word_list)\n",
    "        else:\n",
    "            max_WHL = max(WHL_list)\n",
    "\n",
    "            state = [] # (stateName, wordStateCount, wordTotalCount)\n",
    "            if (WHL_Mel == max_WHL):\n",
    "                state.append([\"Melbourne\", word_counter.Melbourne_word_count[i], word_counter.Melbourne_word_count[i]/word_counter.total_word_count[i]])\n",
    "            elif (WHL_Bri == max_WHL):\n",
    "                state.append([\"Brisbane\", word_counter.Brisbane_word_count[i], word_counter.Brisbane_word_count[i]/word_counter.total_word_count[i]])\n",
    "            elif (WHL_Per == max_WHL):\n",
    "                state.append([\"Perth\", word_counter.Perth_word_count[i], word_counter.Perth_word_count[i]/word_counter.total_word_count[i]])\n",
    "            elif (WHL_Syd == max_WHL):\n",
    "                state.append([\"Sydney\", word_counter.Sydney_word_count[i], word_counter.Sydney_word_count[i]/word_counter.total_word_count[i]])\n",
    "            curr_word_list.append(max_WHL)\n",
    "            curr_word_list.append(i)\n",
    "            curr_word_list.append(curr_word)\n",
    "            curr_word_list.append(state)\n",
    "            WLH_list_one_state.append(curr_word_list)\n",
    "    return WLH_list_occur_more_than_one_state, WLH_list_one_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-19T12:59:41.958823Z",
     "start_time": "2019-05-19T12:59:41.952807Z"
    }
   },
   "outputs": [],
   "source": [
    "def groupby_state(WLHs):\n",
    "    WLH_groupby_state = WLH_lists()\n",
    "        # Initializer / Instance Attributes\n",
    "\n",
    "        \n",
    "    for WLH in WLHs:\n",
    "        state = WLH[3][0][0]\n",
    "        if state == \"Melbourne\":\n",
    "            WLH_groupby_state.Melbourne_WLH_list.append(WLH)\n",
    "        elif state == \"Sydney\":\n",
    "            WLH_groupby_state.Sydney_WLH_list.append(WLH)\n",
    "        elif state == \"Perth\":\n",
    "            WLH_groupby_state.Perth_WLH_list.append(WLH)            \n",
    "        elif state == \"Brisbane\":\n",
    "            WLH_groupby_state.Brisbane_WLH_list.append(WLH)\n",
    "    sorted(WLH_groupby_state.Melbourne_WLH_list, key=lambda x: x[0], reverse=True)\n",
    "    sorted(WLH_groupby_state.Sydney_WLH_list, key=lambda x: x[0], reverse=True)\n",
    "    sorted(WLH_groupby_state.Perth_WLH_list, key=lambda x: x[0], reverse=True)\n",
    "    sorted(WLH_groupby_state.Brisbane_WLH_list, key=lambda x: x[0], reverse=True)\n",
    "    return WLH_groupby_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Top k WHL for each state "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-19T12:59:42.654828Z",
     "start_time": "2019-05-19T12:59:42.649814Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_top_k_index(state_WLH_list, k):\n",
    "    indice = []\n",
    "    WLH_top_k = (sorted(state_WLH_list, key=lambda x: x[0], reverse=True))[:k]\n",
    "    \n",
    "    for WLH in WLH_top_k:\n",
    "        indice.append(WLH[1])\n",
    "    return indice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-19T12:59:42.958945Z",
     "start_time": "2019-05-19T12:59:42.953961Z"
    }
   },
   "outputs": [],
   "source": [
    "# select cloumn from X_train by top 10 index for each state\n",
    "def extract_column_by_index(matrix, indice, vectoriser):\n",
    "    selected_words = []\n",
    "    for index in indice:\n",
    "        selected_words.append(vectoriser.get_feature_names()[index])\n",
    "    \n",
    "    result = []\n",
    "    length = matrix.shape[0]\n",
    "    for i in list(range(length)):\n",
    "        row = matrix[i, :].toarray().sum(axis = 0)\n",
    "        extract = []\n",
    "        for index in indice:\n",
    "            extract.append(row[index])\n",
    "        result.append(extract)\n",
    "    return result, selected_words\n",
    "# feature engineering done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-19T12:59:43.387814Z",
     "start_time": "2019-05-19T12:59:43.380834Z"
    }
   },
   "outputs": [],
   "source": [
    "# This function take the filepath of the text and return a list of preprocessed corpus\n",
    "def preprocess_corpus(filepath):\n",
    "    # read the file into two parts, text and its location\n",
    "    X_raw, y_train = read_tsv(filepath)\n",
    "\n",
    "    # remove URL, hash tag and metion\n",
    "    remove_URL(X_raw)\n",
    "    remove_hash(X_raw)\n",
    "    remove_metion(X_raw)\n",
    "\n",
    "    # remian only letters and numbers\n",
    "    processed_tweets = [re.sub('[^a-zA-Z-0-9]', ' ', tweet) for tweet in X_raw]\n",
    "\n",
    "    # make it into one string and out it into a list => the input format\n",
    "    processed_tweets_in_one = \"\"\n",
    "    for tweet in processed_tweets:\n",
    "        processed_tweets_in_one += tweet\n",
    "    processed_tweets_in_one = processed_tweets_in_one.lower()\n",
    "    processed_tweets_in_one = [processed_tweets_in_one]\n",
    "\n",
    "    # extract each word and make then into 1 list\n",
    "    all_words = [nltk.word_tokenize(tweet) for tweet in processed_tweets_in_one]\n",
    "\n",
    "    # Removing Stop Words\n",
    "    for i in range(len(all_words)):  \n",
    "        all_words[i] = [w for w in all_words[i] if w not in stopwords.words('english')]\n",
    "    \n",
    "    return all_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put all together => Feature Engineering Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-19T12:59:43.767601Z",
     "start_time": "2019-05-19T12:59:43.758595Z"
    }
   },
   "outputs": [],
   "source": [
    "#This function takes in \n",
    "#@param: filepath => The filepath of the corpus\n",
    "def get_top_k_index(filepath):\n",
    "    # read the file into two parts, text and its location\n",
    "    X_raw, y_train = read_tsv(filepath)\n",
    "    \n",
    "    # remove URL, hash tag and metion\n",
    "    remove_URL(X_raw)\n",
    "    remove_hash(X_raw)\n",
    "    remove_metion(X_raw)\n",
    "    \n",
    "    # initilaze vectoriser\n",
    "    vectoriser = CountVectorizer(stop_words=\"english\", min_df = 10 )\n",
    "    X_train = vectoriser.fit_transform(X_raw)\n",
    "    \n",
    "    # calculating WLH\n",
    "    word_counter = word_frequency_counter(X_train, y_train)\n",
    "\n",
    "    # count frequency for each word by state\n",
    "    WLH_list_occur_more_than_one_state, WLH_list_one_state = calculate_WHL_by_state(word_counter, vectoriser)\n",
    "\n",
    "    # group WLH by state\n",
    "    WLH_lists_gourpby_state = groupby_state(WLH_list_occur_more_than_one_state)\n",
    "    \n",
    "    # select top k WHL\n",
    "#     num_features = len(vectoriser.get_feature_names())\n",
    "#     percent = 0.14\n",
    "#     k = percent*num_features/4\n",
    "#     k = int(k)\n",
    "    k = 30\n",
    "    \n",
    "    # extract top 10 WLH index for each state\n",
    "    top_k_indice_for_each_state = []\n",
    "    top_k_indice_for_each_state.extend(extract_top_k_index(WLH_lists_gourpby_state.Brisbane_WLH_list, k))\n",
    "    top_k_indice_for_each_state.extend(extract_top_k_index(WLH_lists_gourpby_state.Melbourne_WLH_list, k))\n",
    "    top_k_indice_for_each_state.extend(extract_top_k_index(WLH_lists_gourpby_state.Perth_WLH_list, k))\n",
    "    top_k_indice_for_each_state.extend(extract_top_k_index(WLH_lists_gourpby_state.Sydney_WLH_list, k))\n",
    "    \n",
    "    return top_k_indice_for_each_state, vectoriser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-19T12:59:44.237521Z",
     "start_time": "2019-05-19T12:59:44.232536Z"
    }
   },
   "outputs": [],
   "source": [
    "# select cloumn from X_train by top 10 index for each state\n",
    "def get_selected_words(indice, vectoriser):\n",
    "    return [vectoriser.get_feature_names()[index] for index in indice]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-19T12:59:44.608712Z",
     "start_time": "2019-05-19T12:59:44.604712Z"
    }
   },
   "outputs": [],
   "source": [
    "# input a sting of tweet, return a list of words cotain in this tweet, remove metion, hash tags and URL,\n",
    "# only include letters and  numbers.\n",
    "def preprocess_single_tweet(tweet):\n",
    "    tweet = re.sub(r\"http\\S+\", \"\", tweet)\n",
    "    tweet = re.sub(r\"@\\S+\", \"\", tweet)\n",
    "    tweet = re.sub(r\"@\", \"\", tweet)\n",
    "    tweet = re.sub(r\"#\\S+\", \"\", tweet)\n",
    "    tweet = re.sub(r\"#\", \"\", tweet)\n",
    "    tweet = re.sub('[^a-zA-Z-0-9]', ' ', tweet)\n",
    "    tweet = tweet.lower()\n",
    "    tweet = nltk.word_tokenize(tweet)\n",
    "    tweet =  [w for w in tweet if w not in stopwords.words('english')]\n",
    "    \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-19T12:59:45.000361Z",
     "start_time": "2019-05-19T12:59:44.996372Z"
    }
   },
   "outputs": [],
   "source": [
    "def raw_to_train(filepath,selected_words):\n",
    "    # read the file into two parts, text and its location\n",
    "    X_raw, y_train = read_tsv(filepath)\n",
    "    \n",
    "    X_train = []\n",
    "    for tweet in X_raw:\n",
    "        # preprocess tweet, remove hash tage, metion, url and only maintain letters and numbers\n",
    "        text = preprocess_single_tweet(tweet)\n",
    "        \n",
    "        #convert tweet from text into trainning instance\n",
    "#         X_train.append(words_to_train_instance_sum_all(selected_words, text))\n",
    "#         X_train.append(words_to_train_instance_sum_top_k(selected_words, text, 1))\n",
    "        X_train.append(words_to_train_instance_sum_larger_than_k(selected_words, text, 0.2))\n",
    "    return X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-19T12:59:45.358412Z",
     "start_time": "2019-05-19T12:59:45.354425Z"
    }
   },
   "outputs": [],
   "source": [
    "def print_0_percet(X, y):\n",
    "    num_instance = len(X)\n",
    "    X_no_zero, y_no_zero = remove_all_zero_instance(X, y)\n",
    "\n",
    "    num_all_zero_instance = num_instance - len(X_no_zero)\n",
    "    print(\"The size of total dataset is %d\"%num_instance)\n",
    "    print(\"The size of all zero instance in this dataset is %d\"%num_all_zero_instance)\n",
    "    print(\"The ratio of all zero instances is %f \"%(num_all_zero_instance/num_instance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use Word2Vec and WLH to extract information "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Strategies 1: for each word sum similarity of all words in tweet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-19T12:59:46.220057Z",
     "start_time": "2019-05-19T12:59:46.215040Z"
    }
   },
   "outputs": [],
   "source": [
    "def words_to_train_instance_sum_all(selected_words, tweet):\n",
    "    single_instance = [0]*len(selected_words)\n",
    "    for i in list(range(len(selected_words))):\n",
    "        score = 0\n",
    "        selected_word = selected_words[i]\n",
    "        for word in tweet:\n",
    "            if (word2vec.wv.vocab.get(word)!=None):\n",
    "                score += word2vec.wv.similarity(word, selected_word)\n",
    "        single_instance[i] = score\n",
    "    return single_instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Strategies 2: for each word sum top k similarity of all words in tweet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-19T12:59:46.563504Z",
     "start_time": "2019-05-19T12:59:46.557521Z"
    }
   },
   "outputs": [],
   "source": [
    "def words_to_train_instance_sum_top_k(selected_words, tweet, k):\n",
    "    single_instance = [0]*len(selected_words)\n",
    "    for i in list(range(len(selected_words))):\n",
    "        score = []\n",
    "        selected_word = selected_words[i]\n",
    "        for word in tweet:\n",
    "            if (word2vec.wv.vocab.get(word)!=None):\n",
    "                score.append(word2vec.wv.similarity(word, selected_word))\n",
    "        score.sort(reverse=True)\n",
    "        single_instance[i] = sum(score[:k])\n",
    "    return single_instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Strategies 3: for each word add > k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-19T12:59:47.217224Z",
     "start_time": "2019-05-19T12:59:47.211240Z"
    }
   },
   "outputs": [],
   "source": [
    "def words_to_train_instance_sum_larger_than_k(selected_words, tweet, k):\n",
    "    single_instance = [0]*len(selected_words)\n",
    "    for i in list(range(len(selected_words))):\n",
    "        score = []\n",
    "        selected_word = selected_words[i]\n",
    "        for word in tweet:\n",
    "            if (word2vec.wv.vocab.get(word)!=None):\n",
    "                if (word2vec.wv.similarity(word, selected_word) > k):\n",
    "                    score.append(word2vec.wv.similarity(word, selected_word))\n",
    "        score.sort(reverse=True)\n",
    "        single_instance[i] = sum(score)\n",
    "    return single_instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-19T12:59:48.435300Z",
     "start_time": "2019-05-19T12:59:48.431310Z"
    }
   },
   "outputs": [],
   "source": [
    "# train_corpus = preprocess_corpus(train_raw_filepath)\n",
    "# dev_corpus = preprocess_corpus(dev_raw_filepath)\n",
    "# test_corpus = preprocess_corpus(test_raw_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-19T12:59:48.780705Z",
     "start_time": "2019-05-19T12:59:48.776715Z"
    }
   },
   "outputs": [],
   "source": [
    "# word2vec = Word2Vec([train_corpus[0],dev_corpus[0], test_corpus[0]] , min_count=2) \n",
    "# word2vec.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-19T12:59:50.042538Z",
     "start_time": "2019-05-19T12:59:49.464087Z"
    }
   },
   "outputs": [],
   "source": [
    "word2vec = Word2Vec.load(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-19T12:59:50.411305Z",
     "start_time": "2019-05-19T12:59:50.406316Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51242"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2vec.wv.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-19T13:01:04.103837Z",
     "start_time": "2019-05-19T12:59:54.300252Z"
    }
   },
   "outputs": [],
   "source": [
    "top_k_indice, vectoriser = get_top_k_index(train_raw_filepath)\n",
    "selected_words = get_selected_words(top_k_indice, vectoriser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-19T09:06:09.432787Z",
     "start_time": "2019-05-19T09:06:09.427799Z"
    }
   },
   "outputs": [],
   "source": [
    "train_small_filepath = \"./train_raw_small.csv\"\n",
    "dev_small_filepath = \"./dev_raw_small.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-19T13:28:10.520924Z",
     "start_time": "2019-05-19T13:01:04.112710Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, y_train = raw_to_train(train_raw_filepath, selected_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-19T09:59:23.212947Z",
     "start_time": "2019-05-19T09:59:23.206963Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103364"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print percetage of all 0 instances "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-19T13:28:10.922768Z",
     "start_time": "2019-05-19T13:28:10.522837Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of total dataset is 103364\n",
      "The size of all zero instance in this dataset is 6874\n",
      "The ratio of all zero instances is 0.066503 \n"
     ]
    }
   ],
   "source": [
    "print_0_percet(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-19T09:09:10.284013Z",
     "start_time": "2019-05-19T09:08:26.833313Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of total dataset is 2550\n",
      "The size of all zero instance in this dataset is 118\n",
      "The ratio of all zero instances is 0.046275 \n"
     ]
    }
   ],
   "source": [
    "X_dev, y_dev = raw_to_train(dev_small_filepath, selected_words)\n",
    "print_0_percet(X_dev, y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-19T13:49:09.398034Z",
     "start_time": "2019-05-19T13:28:10.924774Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of total dataset is 108148\n",
      "The size of all zero instance in this dataset is 6868\n",
      "The ratio of all zero instances is 0.063506 \n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = raw_to_train(test_raw_filepath, selected_words)\n",
    "print_0_percet(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-19T13:49:09.406013Z",
     "start_time": "2019-05-19T13:49:09.400028Z"
    }
   },
   "outputs": [],
   "source": [
    "models = [DummyClassifier(strategy='most_frequent'),\n",
    "          KNeighborsClassifier(n_neighbors=2, algorithm='auto', metric = \"cosine\"),\n",
    "          LogisticRegression()]\n",
    "titles = ['Zero-R',\n",
    "          \"K-NN with cosine similarity\",\n",
    "          'Logistic Regression']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-19T14:56:10.509280Z",
     "start_time": "2019-05-19T13:49:09.409006Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-R 0.25 time: 10.085243940353394\n",
      "K-NN with cosine similarity 0.3077960488888623 time: 3946.5098209381104\n",
      "Logistic Regression 0.29201660568533633 time: 64.40976023674011\n"
     ]
    }
   ],
   "source": [
    "# read csv file\n",
    "# i = 1\n",
    "# train_X, train_y = load_dataset(train_filepath[i])\n",
    "# X_dev_no_0, y_dev_no_0 = remove_all_zero_instance(X_dev, y_dev)\n",
    "# dev_X, dev_y = load_dataset(dev_filepath[i])\n",
    "\n",
    "\n",
    "# try each model without feature selection\n",
    "for title, model in zip(titles, models):\n",
    "    start = time.time()\n",
    "#     model.fit(X_train, y_train)\n",
    "#     acc = np.mean(cross_val_score(model, X_dev, y_dev, cv=10))\n",
    "    acc = np.mean(cross_val_score(model, X_train, y_train, cv=10))\n",
    "    end = time.time()\n",
    "    t = end - start\n",
    "    print(title, acc, 'time:', t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Methods "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-19T09:07:46.280738Z",
     "start_time": "2019-05-19T09:07:46.076973Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "class StackingClassifier():\n",
    "\n",
    "    def __init__(self, classifiers, metaclassifier):\n",
    "        self.classifiers = classifiers\n",
    "        self.metaclassifier = metaclassifier\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        for clf in self.classifiers:\n",
    "            clf.fit(X, y)\n",
    "        X_meta = self._predict_base(X)\n",
    "        self.metaclassifier.fit(X_meta, y)\n",
    "    \n",
    "    def _predict_base(self, X):\n",
    "        yhats = []\n",
    "        for clf in self.classifiers:\n",
    "            yhat = clf.predict_proba(X)\n",
    "            yhats.append(yhat)\n",
    "        yhats = np.concatenate(yhats, axis=1)\n",
    "        #print(yhats.shape)\n",
    "        assert len(yhats) == len(X)\n",
    "        return yhats\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_meta = self._predict_base(X)\n",
    "        yhat = self.metaclassifier.predict(X_meta)\n",
    "        return yhat\n",
    "    def score(self, X, y):\n",
    "        yhat = self.predict(X)\n",
    "        return accuracy_score(y, yhat)\n",
    "    \n",
    "\n",
    "\n",
    "classifiers = [LogisticRegression(),\n",
    "                KNeighborsClassifier(),\n",
    "                GaussianNB(),\n",
    "                MultinomialNB(),\n",
    "                DecisionTreeClassifier()]\n",
    "\n",
    "meta_classifier = DecisionTreeClassifier()\n",
    "stacker = StackingClassifier(classifiers, meta_classifier)\n",
    "\n",
    "def load_car_data(car_file):\n",
    "    X = []\n",
    "    y = []\n",
    "    with open(car_file, mode='r') as fin:\n",
    "        for line in fin:\n",
    "            atts = line.strip().split(\",\")\n",
    "            X.append(atts[:-1]) #all atts minus the last one\n",
    "            y.append(atts[-1])\n",
    "    onehot = OneHotEncoder()\n",
    "    X = onehot.fit_transform(X).toarray()\n",
    "    return X, y\n",
    "# X, y = load_car_data('car.data')\n",
    "# print('labels:', set(y))\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "# stacker.fit(X_train, y_train)\n",
    "# print('stacker acc:', stacker.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-19T09:07:48.280584Z",
     "start_time": "2019-05-19T09:07:47.070203Z"
    }
   },
   "outputs": [],
   "source": [
    "stacker.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-19T09:09:33.454953Z",
     "start_time": "2019-05-19T09:09:32.044724Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stacker acc on train: 0.9534422981674097\n",
      "stacker cross-val acc  on train: 0.27487492903356514\n"
     ]
    }
   ],
   "source": [
    "print('stacker acc on train:', stacker.score(X_train, y_train))\n",
    "cv = 5\n",
    "print('stacker cross-val acc  on train:', np.mean(cross_val_score(stacker.metaclassifier, X_train, y_train, cv=cv)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-19T09:09:37.624321Z",
     "start_time": "2019-05-19T09:09:36.457877Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stacker acc on dev: 0.2411764705882353\n"
     ]
    }
   ],
   "source": [
    "print('stacker acc on dev:', stacker.score(X_dev,y_dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Label Output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =  LogisticRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_labels = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-19T14:56:14.485958Z",
     "start_time": "2019-05-19T13:00:35.294Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_prediction_to_submit_file(test_lables, filepath):\n",
    "    with open(filepath, 'w',  newline='') as writeFile:\n",
    "        writer = csv.writer(writeFile)\n",
    "        writer.writerows([[\"Id\", \"Class\"]])\n",
    "        for i in list(range(len(test_lables))):\n",
    "            index = '3' + str(i+1)\n",
    "            writer.writerows([[index, test_lables[i]]])\n",
    "        writeFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-19T14:56:14.488949Z",
     "start_time": "2019-05-19T13:00:40.592Z"
    }
   },
   "outputs": [],
   "source": [
    "test_prediction_to_submit_file(predict_labels, \"predicted_labels.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "327.639px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
