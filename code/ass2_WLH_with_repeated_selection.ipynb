{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T04:30:21.485662Z",
     "start_time": "2019-05-24T04:30:07.663407Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "m7Z2_D4j5cHt"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import time\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords  \n",
    "import nltk\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "import sys\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_M6wnikH5cHy"
   },
   "source": [
    "# Read raw text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T04:30:21.495342Z",
     "start_time": "2019-05-24T04:30:21.489666Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "SiY125sk5cHz"
   },
   "outputs": [],
   "source": [
    "train_raw_filepath =  \"./2019S1-proj2-data_dos/train-raw.tsv\"\n",
    "dev_raw_filepath =  \"./2019S1-proj2-data_dos/dev-raw.tsv\"\n",
    "test_raw_filepath =  \"./2019S1-proj2-data_dos/test-raw.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T04:30:21.510607Z",
     "start_time": "2019-05-24T04:30:21.499625Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "lE1X7o2u5cH1"
   },
   "outputs": [],
   "source": [
    "# read tsv file into a 2D array\n",
    "def read_tsv(filepath):\n",
    "    label, text = [], []\n",
    "    with open(filepath) as raw:\n",
    "        reader = csv.reader(raw, delimiter=\"\\t\", quoting = csv.QUOTE_NONE)\n",
    "        for row in reader:\n",
    "            label.append(row[1])\n",
    "            text.append(row[-1])\n",
    "    return text, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rG1tXIVZ5cH3"
   },
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T04:30:21.527600Z",
     "start_time": "2019-05-24T04:30:21.514584Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "0OVtwi9i5cH4"
   },
   "outputs": [],
   "source": [
    "def is_all_zero(instance):\n",
    "    for attr in instance:\n",
    "        if int(attr) != 0:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def remove_all_zero_instance(X, y):\n",
    "    new_X, new_y = [], []\n",
    "    for i in range(len(y)):\n",
    "        instance = X[i]\n",
    "        if is_all_zero(instance) is False:\n",
    "            new_X.append(X[i])\n",
    "            new_y.append(y[i])\n",
    "    return new_X, new_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MAAcZ2DN5cH6"
   },
   "source": [
    "## Text Pre-filtering  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T04:30:21.551485Z",
     "start_time": "2019-05-24T04:30:21.533595Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "7Wx2V68N5cH7"
   },
   "outputs": [],
   "source": [
    "def remove_URL(texts):\n",
    "    for i in list(range(len(texts))):\n",
    "        texts[i] = re.sub(r\"http\\S+\", \"\", texts[i])\n",
    "        \n",
    "def remove_metion(texts):\n",
    "    for i in list(range(len(texts))):\n",
    "        texts[i] = re.sub(r\"@\\S+\", \"\", texts[i])\n",
    "        texts[i] = re.sub(r\"@\", \"\", texts[i])\n",
    "        \n",
    "def remove_hash(texts):\n",
    "    for i in list(range(len(texts))):\n",
    "        texts[i] = re.sub(r\"#\\S+\", \"\", texts[i])\n",
    "        texts[i] = re.sub(r\"#\", \"\", texts[i])\n",
    "\n",
    "def remove_unicode(texts):\n",
    "    for i in list(range(len(texts))):\n",
    "        texts[i] = re.sub(r\"\\\\u[a-zA-Z0-9][a-zA-Z0-9][a-zA-Z0-9][a-zA-Z0-9]\", \"\", texts[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T04:30:22.657527Z",
     "start_time": "2019-05-24T04:30:21.555507Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "Pl9P0Kdn5cH9"
   },
   "outputs": [],
   "source": [
    "# a table structure to hold the different punctuation used\n",
    "tbl = dict.fromkeys(i for i in range(sys.maxunicode)\n",
    "                    if unicodedata.category(chr(i)).startswith('P'))\n",
    "# method to remove punctuations from sentences.\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(tbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1CQJHrv35cH_"
   },
   "source": [
    "## WHL calculating "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T04:30:22.668523Z",
     "start_time": "2019-05-24T04:30:22.659953Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "oxvNrcJh5cH_"
   },
   "outputs": [],
   "source": [
    "class Word_Counter:\n",
    "    # Initializer / Instance Attributes\n",
    "    def __init__(self, size):\n",
    "        self.total_word_count = [0]*size\n",
    "        self.Melbourne_word_count = [0]*size\n",
    "        self.Sydney_word_count = [0]*size \n",
    "        self.Perth_word_count = [0]*size        \n",
    "        self.Brisbane_word_count = [0]*size        \n",
    "class WLH_lists:\n",
    "    # Initializer / Instance Attributes\n",
    "    def __init__(self):\n",
    "        self.Melbourne_WLH_list = []\n",
    "        self.Sydney_WLH_list = [] \n",
    "        self.Perth_WLH_list = []        \n",
    "        self.Brisbane_WLH_list = []        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T04:30:22.693458Z",
     "start_time": "2019-05-24T04:30:22.671597Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "8IiZjncA5cIC"
   },
   "outputs": [],
   "source": [
    "def word_frequency_counter(counts, lables):\n",
    "    # create a list to store count\n",
    "    num_words = counts.shape[1]\n",
    "    counter = Word_Counter(num_words)\n",
    "    \n",
    "    for i in list(range(len(lables))):\n",
    "        instance = counts[i, :]\n",
    "        if (lables[i] == \"Melbourne\"):\n",
    "            counter.Melbourne_word_count += instance.toarray().sum(axis=0)\n",
    "        elif (lables[i] == \"Sydney\"):\n",
    "            counter.Sydney_word_count  += instance.toarray().sum(axis=0)\n",
    "        elif (lables[i] == \"Perth\"):\n",
    "            counter.Perth_word_count += instance.toarray().sum(axis=0)\n",
    "        elif (lables[i] == \"Brisbane\"):\n",
    "            counter.Brisbane_word_count += instance.toarray().sum(axis=0)\n",
    "    \n",
    "    counter.total_word_count = counter.Melbourne_word_count + \\\n",
    "                                counter.Sydney_word_count +\\\n",
    "                                counter.Perth_word_count +\\\n",
    "                                counter.Brisbane_word_count\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T04:30:22.732329Z",
     "start_time": "2019-05-24T04:30:22.697422Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "41fi1SaO5cIE"
   },
   "outputs": [],
   "source": [
    "def calculate_WHL_by_state(word_counter, vectoriser):\n",
    "    WLH_list_occur_more_than_one_state = []\n",
    "    WLH_list_one_state = []\n",
    "    \n",
    "    #  total number of word used for each word counter\n",
    "    Melbourne_total_word_num = word_counter.Melbourne_word_count.sum()\n",
    "    Brisbane_total_word_num = word_counter.Brisbane_word_count.sum()\n",
    "    Sydney_total_word_num = word_counter.Sydney_word_count.sum()\n",
    "    Perth_total_word_num = word_counter.Perth_word_count.sum()\n",
    "    all_state_total_word_num = word_counter.total_word_count.sum()\n",
    "\n",
    "    for i in list(range(len(word_counter.total_word_count))):    \n",
    "    # for i in list(range(100)):\n",
    "        curr_word_list = [] # [WHL, word, frequency,  state]\n",
    "        curr_word = vectoriser.get_feature_names()[i]\n",
    "\n",
    "        curr_word_total_prob = word_counter.total_word_count[i]/all_state_total_word_num\n",
    "\n",
    "        WHL_Mel = (word_counter.Melbourne_word_count[i]/Melbourne_total_word_num)/curr_word_total_prob\n",
    "        WHL_Syd = (word_counter.Sydney_word_count[i]/Sydney_total_word_num)/curr_word_total_prob\n",
    "        WHL_Per = (word_counter.Perth_word_count[i]/Perth_total_word_num)/curr_word_total_prob\n",
    "        WHL_Bri = (word_counter.Brisbane_word_count[i]/Brisbane_total_word_num)/curr_word_total_prob\n",
    "\n",
    "        WHL_list = [WHL_Mel, WHL_Bri, WHL_Per, WHL_Syd]\n",
    "        if (occur_in_n_state(WHL_list) > 0):\n",
    "            max_WHL = max(WHL_list)\n",
    "\n",
    "            state = [] # (stateName, wordStateCount, wordTotalCount)\n",
    "            if (WHL_Mel == max_WHL):\n",
    "                state.append([\"Melbourne\", word_counter.Melbourne_word_count[i], word_counter.Melbourne_word_count[i]/word_counter.total_word_count[i]])\n",
    "            if (WHL_Bri == max_WHL):\n",
    "                state.append([\"Brisbane\", word_counter.Brisbane_word_count[i], word_counter.Brisbane_word_count[i]/word_counter.total_word_count[i]])\n",
    "            if (WHL_Per == max_WHL):\n",
    "                state.append([\"Perth\", word_counter.Perth_word_count[i], word_counter.Perth_word_count[i]/word_counter.total_word_count[i]])\n",
    "            if (WHL_Syd == max_WHL):\n",
    "                state.append([\"Sydney\", word_counter.Sydney_word_count[i], word_counter.Sydney_word_count[i]/word_counter.total_word_count[i]])\n",
    "            curr_word_list.append(max_WHL)\n",
    "            curr_word_list.append(i)\n",
    "            curr_word_list.append(curr_word)\n",
    "            curr_word_list.append(state)\n",
    "            WLH_list_occur_more_than_one_state.append(curr_word_list)\n",
    "        else:\n",
    "            max_WHL = max(WHL_list)\n",
    "\n",
    "            state = [] # (stateName, wordStateCount, wordTotalCount)\n",
    "            if (WHL_Mel == max_WHL):\n",
    "                state.append([\"Melbourne\", word_counter.Melbourne_word_count[i], word_counter.Melbourne_word_count[i]/word_counter.total_word_count[i]])\n",
    "            elif (WHL_Bri == max_WHL):\n",
    "                state.append([\"Brisbane\", word_counter.Brisbane_word_count[i], word_counter.Brisbane_word_count[i]/word_counter.total_word_count[i]])\n",
    "            elif (WHL_Per == max_WHL):\n",
    "                state.append([\"Perth\", word_counter.Perth_word_count[i], word_counter.Perth_word_count[i]/word_counter.total_word_count[i]])\n",
    "            elif (WHL_Syd == max_WHL):\n",
    "                state.append([\"Sydney\", word_counter.Sydney_word_count[i], word_counter.Sydney_word_count[i]/word_counter.total_word_count[i]])\n",
    "            curr_word_list.append(max_WHL)\n",
    "            curr_word_list.append(i)\n",
    "            curr_word_list.append(curr_word)\n",
    "            curr_word_list.append(state)\n",
    "            WLH_list_one_state.append(curr_word_list)\n",
    "    return WLH_list_occur_more_than_one_state, WLH_list_one_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T04:30:22.749296Z",
     "start_time": "2019-05-24T04:30:22.736545Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "CanV1IAq5cIG"
   },
   "outputs": [],
   "source": [
    "def groupby_state(WLHs):\n",
    "    WLH_groupby_state = WLH_lists()\n",
    "        # Initializer / Instance Attributes\n",
    "\n",
    "        \n",
    "    for WLH in WLHs:\n",
    "        state = WLH[3][0][0]\n",
    "        if state == \"Melbourne\":\n",
    "            WLH_groupby_state.Melbourne_WLH_list.append(WLH)\n",
    "        elif state == \"Sydney\":\n",
    "            WLH_groupby_state.Sydney_WLH_list.append(WLH)\n",
    "        elif state == \"Perth\":\n",
    "            WLH_groupby_state.Perth_WLH_list.append(WLH)            \n",
    "        elif state == \"Brisbane\":\n",
    "            WLH_groupby_state.Brisbane_WLH_list.append(WLH)\n",
    "    sorted(WLH_groupby_state.Melbourne_WLH_list, key=lambda x: x[0], reverse=True)\n",
    "    sorted(WLH_groupby_state.Sydney_WLH_list, key=lambda x: x[0], reverse=True)\n",
    "    sorted(WLH_groupby_state.Perth_WLH_list, key=lambda x: x[0], reverse=True)\n",
    "    sorted(WLH_groupby_state.Brisbane_WLH_list, key=lambda x: x[0], reverse=True)\n",
    "    return WLH_groupby_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T04:30:22.762249Z",
     "start_time": "2019-05-24T04:30:22.753346Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "S7fSOPEC5cIJ"
   },
   "outputs": [],
   "source": [
    "def occur_in_n_state(WHL_list):\n",
    "    count = 0\n",
    "    for WHL in WHL_list:\n",
    "        if WHL > 0:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T09:07:09.018400Z",
     "start_time": "2019-05-21T09:07:09.014410Z"
    },
    "colab_type": "text",
    "id": "TN3gFXUr5cIL"
   },
   "source": [
    "## Extracting Top k WHL for each state "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T04:30:22.776251Z",
     "start_time": "2019-05-24T04:30:22.766238Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "e0Orum5G5cIM"
   },
   "outputs": [],
   "source": [
    "def extract_top_k_index(state_WLH_list, k):\n",
    "    indice = []\n",
    "    WLH_top_k = (sorted(state_WLH_list, key=lambda x: x[0], reverse=True))[:k]\n",
    "    \n",
    "    for WLH in WLH_top_k:\n",
    "        indice.append(WLH[1])\n",
    "    return indice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T04:30:22.794163Z",
     "start_time": "2019-05-24T04:30:22.781200Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "BIrr_VYk5cIO"
   },
   "outputs": [],
   "source": [
    "# select cloumn from X_train by top 10 index for each state\n",
    "def extract_column_by_index(matrix, indice, vectoriser):\n",
    "    selected_words = []\n",
    "    for index in indice:\n",
    "        selected_words.append(vectoriser.get_feature_names()[index])\n",
    "    \n",
    "    result = []\n",
    "    length = matrix.shape[0]\n",
    "    for i in list(range(length)):\n",
    "        row = matrix[i, :].toarray().sum(axis = 0)\n",
    "        extract = []\n",
    "        for index in indice:\n",
    "            extract.append(row[index])\n",
    "        result.append(extract)\n",
    "    return result, selected_words\n",
    "# feature engineering done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl79jAoN5cIQ"
   },
   "source": [
    "## Put all together => Feature Engineering Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T04:30:22.809123Z",
     "start_time": "2019-05-24T04:30:22.798152Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "EBGtslsU5cIQ"
   },
   "outputs": [],
   "source": [
    "def preprocessing_test(filepath, selected_words):\n",
    "    # read the file into two parts, text and its location\n",
    "    X_raw, y = read_tsv(filepath)\n",
    "    \n",
    "    # remove URL, hash tag and metion\n",
    "    remove_URL(X_raw)\n",
    "    remove_hash(X_raw)\n",
    "    remove_metion(X_raw)\n",
    "    remove_unicode(X_raw)\n",
    "    X_raw = [ remove_punctuation(text) for text in X_raw]\n",
    "        \n",
    "    # initilaze vectoriser\n",
    "    vectoriser = CountVectorizer(stop_words=\"english\", vocabulary=selected_words)\n",
    "    X_sparse = vectoriser.fit_transform(X_raw)\n",
    "    \n",
    "    \n",
    "    # make sparse matrix into 2D list\n",
    "    X = []\n",
    "    for i in list(range(len(y))):\n",
    "        X.append(X_sparse[i, :].toarray().sum(axis = 0))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T04:30:22.847233Z",
     "start_time": "2019-05-24T04:30:22.832062Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "rh7LeqJz5cIS"
   },
   "outputs": [],
   "source": [
    "#This function takes in \n",
    "#@param: filepath => The filepath of the corpus\n",
    "def preprocessing_train(filepath):\n",
    "    # read the file into two parts, text and its location\n",
    "    X_raw, y_train = read_tsv(filepath)\n",
    "    \n",
    "    # remove URL, hash tag and metion\n",
    "    remove_URL(X_raw)\n",
    "    remove_hash(X_raw)\n",
    "    remove_metion(X_raw)\n",
    "    remove_unicode(X_raw)\n",
    "    X_raw = [ remove_punctuation(text) for text in X_raw]\n",
    "    \n",
    "#     # initialize the stemmer\n",
    "#     stemmer = LancasterStemmer()\n",
    "#     X_word_stems = []\n",
    "#     for i in list(range(len(y_train))):\n",
    "#         # stem and lower each word and remove duplicates\n",
    "#         words = [stemmer.stem(w.lower()) for w in X_raw[i]]\n",
    "#         X_word_stems.extend(words)\n",
    "        \n",
    "    # initilaze vectoriser\n",
    "    vectoriser = CountVectorizer(stop_words=\"english\", min_df = 15 )\n",
    "    X_train = vectoriser.fit_transform(X_raw)\n",
    "    \n",
    "    # calculating WLH\n",
    "    word_counter = word_frequency_counter(X_train, y_train)\n",
    "\n",
    "    # count frequency for each word by state\n",
    "    WLH_list_occur_more_than_one_state, WLH_list_one_state = calculate_WHL_by_state(word_counter, vectoriser)\n",
    "\n",
    "    # group WLH by state\n",
    "    WLH_lists_gourpby_state = groupby_state(WLH_list_occur_more_than_one_state)\n",
    "    \n",
    "    # select top k WHL\n",
    "    num_features = len(vectoriser.get_feature_names())\n",
    "    percent = 0.14\n",
    "    k = percent*num_features/4\n",
    "    k = int(k)\n",
    "    k = 300\n",
    "    \n",
    "    # extract top 10 WLH index for each state\n",
    "    top_k_indice_for_each_state = []\n",
    "    top_k_indice_for_each_state.extend(extract_top_k_index(WLH_lists_gourpby_state.Brisbane_WLH_list, k))\n",
    "    top_k_indice_for_each_state.extend(extract_top_k_index(WLH_lists_gourpby_state.Melbourne_WLH_list, k))\n",
    "    top_k_indice_for_each_state.extend(extract_top_k_index(WLH_lists_gourpby_state.Perth_WLH_list, k))\n",
    "    top_k_indice_for_each_state.extend(extract_top_k_index(WLH_lists_gourpby_state.Sydney_WLH_list, k))\n",
    "    \n",
    "    # extract columns\n",
    "    new_train, selected_words = extract_column_by_index(X_train, top_k_indice_for_each_state, vectoriser)\n",
    "    return new_train, y_train, selected_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "D5W6QzFHWqd0"
   },
   "source": [
    "# Repeated Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "jHi9IsQ-K42G"
   },
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "otCifXFnLsBB"
   },
   "outputs": [],
   "source": [
    "def text_preprocessing(filepath):\n",
    "    # read the file into two parts, text and its location\n",
    "    X_raw, y_train = read_tsv(filepath)\n",
    "    \n",
    "    # remove URL, hash tag and metion\n",
    "    remove_URL(X_raw)\n",
    "    remove_hash(X_raw)\n",
    "    remove_metion(X_raw)\n",
    "    remove_unicode(X_raw)\n",
    "    X_raw = [ remove_punctuation(text) for text in X_raw]\n",
    "    \n",
    "    return X_raw, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "q1ASZUxhK5-d"
   },
   "outputs": [],
   "source": [
    "def text_to_train(X_text, y_train):\n",
    "    # initilaze vectoriser\n",
    "    vectoriser = CountVectorizer(stop_words=\"english\", min_df = 10 )\n",
    "    X_train = vectoriser.fit_transform(X_text)\n",
    "    \n",
    "    # calculating WLH\n",
    "    word_counter = word_frequency_counter(X_train, y_train)\n",
    "\n",
    "    # count frequency for each word by state\n",
    "    WLH_list_occur_more_than_one_state, WLH_list_one_state = calculate_WHL_by_state(word_counter, vectoriser)\n",
    "\n",
    "    # group WLH by state\n",
    "    WLH_lists_gourpby_state = groupby_state(WLH_list_occur_more_than_one_state)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # select top k WHL\n",
    "    num_features = len(vectoriser.get_feature_names())\n",
    "    percent = 0.14\n",
    "    k = percent*num_features/4\n",
    "    k = int(k)\n",
    "    k = 200\n",
    "    \n",
    "    # extract top 10 WLH index for each state\n",
    "    top_k_indice_for_each_state = []\n",
    "    top_k_indice_for_each_state.extend(extract_top_k_index(WLH_lists_gourpby_state.Brisbane_WLH_list, k))\n",
    "    top_k_indice_for_each_state.extend(extract_top_k_index(WLH_lists_gourpby_state.Melbourne_WLH_list, k))\n",
    "    top_k_indice_for_each_state.extend(extract_top_k_index(WLH_lists_gourpby_state.Perth_WLH_list, k))\n",
    "    top_k_indice_for_each_state.extend(extract_top_k_index(WLH_lists_gourpby_state.Sydney_WLH_list, k))\n",
    "    \n",
    "    # extract columns\n",
    "    new_train, selected_words = extract_column_by_index(X_train, top_k_indice_for_each_state, vectoriser)\n",
    "    \n",
    "    return new_train, y_train, selected_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "tig4PhngmZCA"
   },
   "outputs": [],
   "source": [
    "def text_to_test(X_text, y_test, selected_words):\n",
    "    # initilaze vectoriser\n",
    "    vectoriser = CountVectorizer(stop_words=\"english\", vocabulary=selected_words)\n",
    "    X_test = vectoriser.fit_transform(X_text)\n",
    "    \n",
    "    new_test = []\n",
    "    \n",
    "    # Add index to X_test => [index, bag-of-word-model]\n",
    "    for i in list(range(len(y_test))):\n",
    "        new_test.append([i, X_test[i, :].toarray().sum(axis=0)])\n",
    "    return new_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T23:09:03.256424Z",
     "start_time": "2019-05-21T23:09:03.250442Z"
    },
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "Zd4cAMyR5cIU"
   },
   "outputs": [],
   "source": [
    "def print_0_percet(X, y):\n",
    "    num_instance = len(X)\n",
    "    X_no_zero, y_no_zero = remove_all_zero_instance(X, y)\n",
    "\n",
    "    num_all_zero_instance = num_instance - len(X_no_zero)\n",
    "    print(\"The size of total dataset is %d\"%num_instance)\n",
    "    print(\"The size of all zero instance in this dataset is %d\"%num_all_zero_instance)\n",
    "    print(\"The ratio of all zero instances is %f \"%(num_all_zero_instance/num_instance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "HCNJntEUG5Xh"
   },
   "outputs": [],
   "source": [
    "def extract_all_zero_train_text(X, X_text, y):\n",
    "    X_text_all_zero = []\n",
    "    y_all_zero = []\n",
    "    X_not_all_zero = []\n",
    "    y_not_all_zero = []\n",
    "    \n",
    "    for i in list(range(len(y))):\n",
    "        if is_all_zero(X[i]):\n",
    "            X_text_all_zero.append(X_text[i])\n",
    "            y_all_zero.append(y[i])\n",
    "        else:\n",
    "            X_text_not_all_zero.append(X[i])\n",
    "            y_not_all_zero.append(y[i])\n",
    "    return X_text_all_zero, y_all_zero, X_not_all_zero, y_not_all_zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "ZyKutNXImHnT"
   },
   "outputs": [],
   "source": [
    "def extract_all_zero_test_text(X, X_text, y):\n",
    "    X_text_all_zero = []\n",
    "    y_all_zero = []\n",
    "    X_not_all_zero = []\n",
    "    y_not_all_zero = []\n",
    "    \n",
    "    for i in list(range(len(y))):\n",
    "        # append index to instance, thus index 1 is trainning instance\n",
    "        if is_all_zero(X[i][1]):\n",
    "            X_text_all_zero.append(X_text[i])\n",
    "            y_all_zero.append(y[i])\n",
    "        else:\n",
    "            X_not_all_zero.append(X[i])\n",
    "            y_not_all_zero.append(y[i])\n",
    "    return X_text_all_zero, y_all_zero, X_not_all_zero, y_not_all_zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "TfaZd0qrW8xN"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "_piZq2h2jYyR"
   },
   "outputs": [],
   "source": [
    "X_train_text, y_train = text_preprocessing(dev_raw_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "PDVEX0hmjdUJ"
   },
   "outputs": [],
   "source": [
    "X_train, y_train, selected_words = text_to_train(X_train_text, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "hidden": true,
    "id": "ozFSJ7EejuFO",
    "outputId": "0e693a71-e039-4ea8-d204-71bbfcddbb67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "ZVFM5VoFm6iE"
   },
   "outputs": [],
   "source": [
    "# split the dataset, extract all zero text\n",
    "X_train_text, y_train, X_train_not_all_zero, y_train_not_all_zero = extract_all_zero_train_text(X_train, X_train_text, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "hidden": true,
    "id": "IXGycFDHnHf3",
    "outputId": "f19a2825-3923-49f0-b2b6-77a381903c68"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 48,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train classifier on X_train_1\n",
    "classifier =  LogisticRegression()\n",
    "classifier.fit(X_train_not_all_zero, y_train_not_all_zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "Fxv90o_WnuYX"
   },
   "outputs": [],
   "source": [
    "# save the classifier 1, selected words 1\n",
    "system = [classifier, selected_words]\n",
    "systems.append(system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "hidden": true,
    "id": "d9yq4VRjmvkZ",
    "outputId": "edb49015-3d14-40cb-cdad-3783e747eaab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7582814132580008\n"
     ]
    }
   ],
   "source": [
    "all_zero_instance_percent = len(y_train)/total_num_of_instance\n",
    "print(all_zero_instance_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "YppzyDvJlzIQ"
   },
   "outputs": [],
   "source": [
    "# text filtering\n",
    "X_train_text, y_train = text_preprocessing(train_raw_filepath)\n",
    "total_num_of_instance = len(y_train)\n",
    "all_zero_instance_percent = 1\n",
    "systems = []\n",
    "#[(classfier_1, selected_words_1), (classfier_2, selected_words_2), .. ,(classfier_n, selected_words_n)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "0eWkEm49l3DA"
   },
   "outputs": [],
   "source": [
    "# text to bag of words model based on WLH, call it X_train_1\n",
    "X_train, y_train, selected_words = text_to_train(X_train_text, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "hidden": true,
    "id": "7lw63VS6l7bS",
    "outputId": "77fa2451-306a-4874-8ce6-7fddaa67a3a4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "268"
      ]
     },
     "execution_count": 125,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 722
    },
    "colab_type": "code",
    "hidden": true,
    "id": "Fwy00HMXSWEO",
    "outputId": "905a3370-843b-4703-9b01-a2013e2e1534"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-186-bb833dd40aca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# train classifier on X_train_1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_not_all_zero\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_not_all_zero\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# save the classifier 1, selected words 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         X, y = check_X_y(X, y, accept_sparse='csr', dtype=_dtype, order=\"C\",\n\u001b[0;32m-> 1532\u001b[0;31m                          accept_large_sparse=solver != 'liblinear')\n\u001b[0m\u001b[1;32m   1533\u001b[0m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    717\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m                     \u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m                     estimator=estimator)\n\u001b[0m\u001b[1;32m    720\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    519\u001b[0m                     \u001b[0;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                     \u001b[0;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0;31m# in the future np.flexible dtypes will be handled like object dtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "# text filtering\n",
    "X_train_text, y_train = text_preprocessing(train_raw_filepath)\n",
    "total_num_of_instance = len(y_train)\n",
    "all_zero_instance_percent = 1\n",
    "systems = []\n",
    "#[(classfier_1, selected_words_1), (classfier_2, selected_words_2), .. ,(classfier_n, selected_words_n)]\n",
    "\n",
    "while all_zero_instance_percent > 0.05:\n",
    "    # text to bag of words model based on WLH, call it X_train_1\n",
    "    X_train, y_train, selected_words = text_to_train(X_train_text, y_train)\n",
    "    \n",
    "    # split the dataset, extract all zero text\n",
    "    X_train_text, y_train, X_train_not_all_zero, y_train_not_all_zero = extract_all_zero_train_text(X_train, X_train_text, y_train)\n",
    "    \n",
    "    \n",
    "    # train classifier on X_train_1\n",
    "    classifier =  LogisticRegression()\n",
    "    classifier.fit(X_train_not_all_zero, y_train_not_all_zero)\n",
    "    \n",
    "    # save the classifier 1, selected words 1\n",
    "    system = [classifier, selected_words]\n",
    "    systems.append(system)\n",
    "    \n",
    "    \n",
    "    all_zero_instance_percent = len(y_train)/total_num_of_instance\n",
    "    print(\"###########################Current all 0 instance percentage: \" + str(all_zero_instance_percent) + \"##########################\")\n",
    "\n",
    "########################################################################\n",
    "    \n",
    "# text to bag of words model based on WLH, call it X_train_2\n",
    "\n",
    "# extract all zero text\n",
    "\n",
    "# train classifier on X_train_2\n",
    "\n",
    "# save the classifier 2, selected words 2\n",
    "\n",
    "# text to bag of words model based on WLH, call it X_train_3\n",
    "\n",
    "# extract all zero text\n",
    "\n",
    "# train classifier on X_train_3\n",
    "\n",
    "# save the classifier 3, selected words 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "hidden": true,
    "id": "cIwzITXloQyA",
    "outputId": "14e90eb5-f4ed-4aab-e0ce-d6d9d1da5b5d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 268)"
      ]
     },
     "execution_count": 142,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(system[0].coef_).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "hidden": true,
    "id": "KCe4jM8HAMJF",
    "outputId": "672fdcfa-e7b3-40fd-8c8a-bed5334a87c2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 268)"
      ]
     },
     "execution_count": 146,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(systems[7][0].coef_).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "hidden": true,
    "id": "7SAZv-OGAhGb",
    "outputId": "ecd60456-6d24-4963-86a2-a247733d9889"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "268"
      ]
     },
     "execution_count": 144,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(systems[7][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "G3_NAoHzW5hE"
   },
   "source": [
    "## Prediction on dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "uP2MuRtGcIiv"
   },
   "outputs": [],
   "source": [
    "def predict(filepath):\n",
    "    # text filtering\n",
    "    X_dev_text, y_dev = text_preprocessing(dev_raw_filepath)\n",
    "    \n",
    "    # prediction\n",
    "    for system in systems:\n",
    "        # text to bag of words model based on WLH, call it X_train_2\n",
    "\n",
    "        # extract all zero text\n",
    "\n",
    "        # train classifier on X_train_2\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "VyjSBgUI6sE9"
   },
   "outputs": [],
   "source": [
    "def predict_the_label(X_test, classifier, predict_labels):\n",
    "    actual_labels = []\n",
    "    indice = []\n",
    "    for i in list(range(len(X_test))):\n",
    "        label = classifier.predict([X_test[i][1]])\n",
    "        index = X_test[i][0]\n",
    "        predict_labels[index] = label\n",
    "        indice.append(index)\n",
    "    return predict_labels, indice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "QwJIknj82Wu0"
   },
   "outputs": [],
   "source": [
    "def is_all_label_stored_corrected(predict_labels, indice):\n",
    "    none_zero_index = []\n",
    "    for i in list(range(len(predict_labels))):\n",
    "        if (predict_labels[i] != 0):\n",
    "            none_zero_index.append(i)\n",
    "    \n",
    "    for i in none_zero_index:\n",
    "        if i not in indice:\n",
    "            return False\n",
    "        \n",
    "    for i in indice:\n",
    "        if i not in none_zero_index:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "tqgIuDxG2a9q"
   },
   "outputs": [],
   "source": [
    "def subsystem_accuracy(predict_labels, actual_labels, indice):\n",
    "    n = len(indice)\n",
    "    num_correct = 0\n",
    "    for index in indice:\n",
    "        if predict_labels[index] == actual_labels[index]:\n",
    "            num_correct += 1\n",
    "    return num_correct/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "MZkR0uPz0H1z"
   },
   "outputs": [],
   "source": [
    "# text filtering\n",
    "X_dev_text, y_dev = text_preprocessing(dev_raw_filepath)\n",
    "# prepare the list for prediction results\n",
    "predict_labels = [0]*len(y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "s0lfooyU9AFX"
   },
   "outputs": [],
   "source": [
    "total_num_instance = len(X_dev_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "SZvWP14u0IGS"
   },
   "outputs": [],
   "source": [
    "# choose the first system\n",
    "system = systems[0]\n",
    "selected_word_0 = system[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "hidden": true,
    "id": "UFc57B1kA7L7",
    "outputId": "4caa1d5e-bd38-43bb-dda4-9b5d9e524828"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 148,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(selected_word_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "rEhg0tTc0ivC"
   },
   "outputs": [],
   "source": [
    "# text to bag of words model based on WLH\n",
    "X_dev, y_dev = text_to_test(X_dev_text, y_dev, selected_word_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "hidden": true,
    "id": "uH2zBTLO5ioj",
    "outputId": "11221e09-608a-4544-e573-27af26cfa928"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37316"
      ]
     },
     "execution_count": 104,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "JrBTIyXY0l1b"
   },
   "outputs": [],
   "source": [
    "# extract all zero text\n",
    "X_text_all_zero, y_all_zero, X_not_all_zero, y_not_all_zero =extract_all_zero_test_text(X_dev, X_dev_text, y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "hidden": true,
    "id": "hrkdaQSs0qVR",
    "outputId": "857175a0-d4e5-461d-b187-19e5c4413559"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent of all zero instance:0.875093793547004\n",
      "Number of all zero instance:32655\n"
     ]
    }
   ],
   "source": [
    "# print out number of all zero instance\n",
    "print(\"Percent of all zero instance:\"  +str(len(y_all_zero)/total_num_instance))\n",
    "print(\"Number of all zero instance:\"  +str(len(y_all_zero)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "hidden": true,
    "id": "8bIdQC_Y94_T",
    "outputId": "1bdbb828-bc4d-476e-93a5-2ca13181ec69"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 400)"
      ]
     },
     "execution_count": 151,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = system[0]\n",
    "(classifier.coef_).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "LiDo7vW19W5i"
   },
   "outputs": [],
   "source": [
    "#make prediction\n",
    "predict_labels, indice = predict_the_label(X_not_all_zero, classifier, predict_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "hidden": true,
    "id": "acfwStUK0nLu",
    "outputId": "8882da9a-feec-4fb6-c226-5b40f5dde8d8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 178,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check whether store the label correctly\n",
    "is_all_label_stored_corrected(predict_labels, indice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "hidden": true,
    "id": "c2f9f0A8CIyC",
    "outputId": "9f30e992-0f10-4b07-8ae0-a02d5c1fe7ad"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6288350139455052"
      ]
     },
     "execution_count": 179,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subsystem_accuracy(predict_labels, y_dev, indice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "2aH1gD5tCPEF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "rZuYn7p5CPAn"
   },
   "outputs": [],
   "source": [
    "# choose the first system\n",
    "system = systems[1]\n",
    "selected_word_0 = system[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "346BAU_CCO80"
   },
   "outputs": [],
   "source": [
    "# text to bag of words model based on WLH\n",
    "X_dev, y_dev = text_to_test(X_dev_text, y_dev, selected_word_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "RIV1wPR00tPd"
   },
   "outputs": [],
   "source": [
    "# print out accuracy of current system\n",
    "X_dev_text, y_dev_fixed = text_preprocessing(dev_raw_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1022
    },
    "colab_type": "code",
    "hidden": true,
    "id": "puIJOEnNW06z",
    "outputId": "8f943071-eab2-41c2-c327-d6f1cc4b6e68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================\n",
      "Percent of all zero instance:0.875093793547004\n",
      "Number of all zero instance:32655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is lable stored correctly:True\n",
      "The accuracy is: 0.6288350139455052\n",
      "============================================================================\n",
      "Percent of all zero instance:0.795503269375067\n",
      "Number of all zero instance:29685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is lable stored correctly:False\n",
      "The accuracy is: 0.2622895622895623\n",
      "============================================================================\n",
      "Percent of all zero instance:0.703799978561475\n",
      "Number of all zero instance:26263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is lable stored correctly:False\n",
      "The accuracy is: 0.24342489772063122\n",
      "============================================================================\n",
      "Percent of all zero instance:0.6002787008253833\n",
      "Number of all zero instance:22400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is lable stored correctly:False\n",
      "The accuracy is: 0.24851151954439554\n",
      "============================================================================\n",
      "Percent of all zero instance:0.4928448922714117\n",
      "Number of all zero instance:18391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is lable stored correctly:False\n",
      "The accuracy is: 0.2596657520578698\n",
      "============================================================================\n",
      "Percent of all zero instance:0.37935470039661273\n",
      "Number of all zero instance:14156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is lable stored correctly:False\n",
      "The accuracy is: 0.2538370720188902\n",
      "============================================================================\n",
      "Percent of all zero instance:0.254394897631043\n",
      "Number of all zero instance:9493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is lable stored correctly:False\n",
      "The accuracy is: 0.24812352562727857\n",
      "============================================================================\n",
      "Percent of all zero instance:0.14420087897952621\n",
      "Number of all zero instance:5381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is lable stored correctly:False\n",
      "The accuracy is: 0.24319066147859922\n"
     ]
    }
   ],
   "source": [
    "# text filtering\n",
    "X_dev_text, y_dev = text_preprocessing(dev_raw_filepath)\n",
    "# prepare the list for prediction results\n",
    "predict_labels = [0]*len(y_dev)\n",
    "\n",
    "for system in systems:\n",
    "    classifier = system[0]\n",
    "    selected_word = system[1]\n",
    "    \n",
    "    # text to bag of words model based on WLH\n",
    "    X_dev, y_dev = text_to_test(X_dev_text, y_dev, selected_word)\n",
    "\n",
    "    # extract all zero text\n",
    "    X_dev_text, y_dev, X_not_all_zero, y_not_all_zero =extract_all_zero_test_text(X_dev, X_dev_text, y_dev)\n",
    "    \n",
    "    print(\"============================================================================\")\n",
    "    # print out number of all zero instance\n",
    "    print(\"Percent of all zero instance:\"  +str(len(y_dev)/total_num_instance))\n",
    "    print(\"Number of all zero instance:\"  +str(len(y_dev)))\n",
    "\n",
    "    #make prediction\n",
    "    predict_labels, indice = predict_the_label(X_not_all_zero, classifier, predict_labels)\n",
    "    \n",
    "    # check whether store the label correctly\n",
    "    print(\"Is lable stored correctly:\" + str(is_all_label_stored_corrected(predict_labels, indice)))\n",
    "    \n",
    "    # print subsystem accuracy\n",
    "    print(\"The accuracy is: \" + str(subsystem_accuracy(predict_labels, y_dev_fixed, indice)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "md63LRBP0zbk"
   },
   "outputs": [],
   "source": [
    "# print overall accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "RP8VrKT9W2z7"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "nr_jL_LKW1gl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XJI3gIUp5cIW"
   },
   "source": [
    "# Main "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i1WZKgLwOXu_"
   },
   "outputs": [],
   "source": [
    "X_train_text, y_train = text_preprocessing(train_raw_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T04:32:54.191249Z",
     "start_time": "2019-05-24T04:30:29.271208Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "XvRI4B_I5cJH"
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-705a16f16567>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mselected_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessing_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_raw_filepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-16-8b6d02ae1c65>\u001b[0m in \u001b[0;36mpreprocessing_train\u001b[1;34m(filepath)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[1;31m# extract columns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m     \u001b[0mnew_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mselected_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_column_by_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtop_k_indice_for_each_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvectoriser\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnew_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mselected_words\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-bfed3ce9d4f0>\u001b[0m in \u001b[0;36mextract_column_by_index\u001b[1;34m(matrix, indice, vectoriser)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mlength\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mrow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mextract\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindice\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\site-packages\\numpy\\core\\_methods.py\u001b[0m in \u001b[0;36m_sum\u001b[1;34m(a, axis, dtype, out, keepdims, initial)\u001b[0m\n\u001b[0;32m     34\u001b[0m def _sum(a, axis=None, dtype=None, out=None, keepdims=False,\n\u001b[0;32m     35\u001b[0m          initial=_NoValue):\n\u001b[1;32m---> 36\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m def _prod(a, axis=None, dtype=None, out=None, keepdims=False,\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_train, y_train, selected_words = preprocessing_train(train_raw_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wK5dOL9_eldu"
   },
   "outputs": [],
   "source": [
    "X_train, y_train = 1, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "8oPS07wzVY__",
    "outputId": "4c070b3c-ca6a-42d2-9779-cb1dd0bc407b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103364"
      ]
     },
     "execution_count": 57,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8yW61pXqGYGu"
   },
   "outputs": [],
   "source": [
    " X_text_all_zero, y_all_zero = extract_all_zero_train_text(X_train, X_train_text, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "bFaQRMYmI9fq",
    "outputId": "c6142d2e-4557-446d-b4fd-23837c9a5e71"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Where my Geminis at'"
      ]
     },
     "execution_count": 117,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_text_all_zero[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T23:13:29.016726Z",
     "start_time": "2019-05-21T23:12:51.092076Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "ZXgRkM0u5cJU",
    "outputId": "a0e5006d-8f67-4c2d-8440-9095218c9ee1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of total dataset is 103364\n",
      "The size of all zero instance in this dataset is 58839\n",
      "The ratio of all zero instances is 0.569241 \n"
     ]
    }
   ],
   "source": [
    "print_0_percet(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T13:09:27.862212Z",
     "start_time": "2019-05-21T13:09:14.287419Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "sguEt_1e5cJX"
   },
   "outputs": [],
   "source": [
    "X_dev, y_dev = preprocessing_test(dev_raw_filepath, selected_words)\n",
    "print_0_percet(X_dev, y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T14:13:28.144356Z",
     "start_time": "2019-05-21T14:12:51.614815Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "fsCvWRBt5cJf",
    "outputId": "1ecb6ae1-c009-4a48-840a-829b85fcf29f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of total dataset is 108148\n",
      "The size of all zero instance in this dataset is 60440\n",
      "The ratio of all zero instances is 0.558864 \n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = preprocessing_test(test_raw_filepath, selected_words)\n",
    "print_0_percet(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pqnt6b6z5cJi"
   },
   "source": [
    "# Model Training & Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aF3Nd94Q5cJj"
   },
   "source": [
    "## DNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T01:01:59.690645Z",
     "start_time": "2019-05-22T01:01:59.671695Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "-Pk_wNY05cJm",
    "outputId": "094db001-c83e-4160-977c-4e72d42c12b3"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-159-64156d691fe5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T00:48:53.913054Z",
     "start_time": "2019-05-22T00:48:53.895102Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "XFWQw2YC5cJq",
    "outputId": "23ba2db3-8ade-4ea8-e7f3-f542782ce5ca"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tflearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-156-29a6fabe2cbd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtflearn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tflearn'"
     ]
    }
   ],
   "source": [
    "import tflearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ou2Oh2vy5cJs"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sSK7n3hj5cJu"
   },
   "source": [
    "## Basic Classfiers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T13:09:27.872154Z",
     "start_time": "2019-05-21T13:09:27.865194Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "BI1XzcOk5cJu"
   },
   "outputs": [],
   "source": [
    "models = [DummyClassifier(strategy='most_frequent'),\n",
    "          GaussianNB(),\n",
    "          MultinomialNB(),\n",
    "          LogisticRegression()]\n",
    "titles = ['Zero-R',\n",
    "          'GNB',\n",
    "          'MNB',\n",
    "          'Logistic Regression']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T13:11:20.487814Z",
     "start_time": "2019-05-21T13:09:27.874150Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 882
    },
    "colab_type": "code",
    "id": "G5fNfUn05cJw",
    "outputId": "5df1ba4b-a6f6-46a7-e826-c406dd354e10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-R 0.25 time: 0.3765854835510254\n",
      "GNB 0.30922499528495 time: 41.52842974662781\n",
      "MNB 0.3284113386601898 time: 25.651182174682617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression 0.3322177640082985 time: 29.649566650390625\n"
     ]
    }
   ],
   "source": [
    "# read csv file\n",
    "# i = 1\n",
    "# train_X, train_y = load_dataset(train_filepath[i])\n",
    "X_dev_no_0, y_dev_no_0 = remove_all_zero_instance(X_dev, y_dev)\n",
    "# dev_X, dev_y = load_dataset(dev_filepath[i])\n",
    "\n",
    "\n",
    "# try each model without feature selection\n",
    "for title, model in zip(titles, models):\n",
    "    start = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    acc = np.mean(cross_val_score(model, X_dev, y_dev, cv=10))\n",
    "#     acc = np.mean(cross_val_score(model, X_train, y_train, cv=10))\n",
    "    end = time.time()\n",
    "    t = end - start\n",
    "    print(title, acc, 'time:', t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zanHbLBk5cJ7"
   },
   "source": [
    "## Stacking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T14:39:35.263814Z",
     "start_time": "2019-05-21T14:39:35.231902Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "WiL0hF1W5cJ8"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "class StackingClassifier():\n",
    "\n",
    "    def __init__(self, classifiers, metaclassifier):\n",
    "        self.classifiers = classifiers\n",
    "        self.metaclassifier = metaclassifier\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        for clf in self.classifiers:\n",
    "            clf.fit(X, y)\n",
    "        X_meta = self._predict_base(X)\n",
    "        self.metaclassifier.fit(X_meta, y)\n",
    "    \n",
    "    def _predict_base(self, X):\n",
    "        yhats = []\n",
    "        for clf in self.classifiers:\n",
    "            yhat = clf.predict_proba(X)\n",
    "            yhats.append(yhat)\n",
    "        yhats = np.concatenate(yhats, axis=1)\n",
    "        #print(yhats.shape)\n",
    "        assert len(yhats) == len(X)\n",
    "        return yhats\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_meta = self._predict_base(X)\n",
    "        yhat = self.metaclassifier.predict(X_meta)\n",
    "        return yhat\n",
    "    def score(self, X, y):\n",
    "        yhat = self.predict(X)\n",
    "        return accuracy_score(y, yhat)\n",
    "    \n",
    "\n",
    "\n",
    "classifiers = [LogisticRegression(),\n",
    "#                 KNeighborsClassifier(),\n",
    "                GaussianNB(),\n",
    "                MultinomialNB(),\n",
    "#                 DecisionTreeClassifier()\n",
    "              ]\n",
    "              \n",
    "meta_classifier = DecisionTreeClassifier()\n",
    "stacker = StackingClassifier(classifiers, meta_classifier)\n",
    "\n",
    "def load_car_data(car_file):\n",
    "    X = []\n",
    "    y = []\n",
    "    with open(car_file, mode='r') as fin:\n",
    "        for line in fin:\n",
    "            atts = line.strip().split(\",\")\n",
    "            X.append(atts[:-1]) #all atts minus the last one\n",
    "            y.append(atts[-1])\n",
    "    onehot = OneHotEncoder()\n",
    "    X = onehot.fit_transform(X).toarray()\n",
    "    return X, y\n",
    "# X, y = load_car_data('car.data')\n",
    "# print('labels:', set(y))\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "# stacker.fit(X_train, y_train)\n",
    "# print('stacker acc:', stacker.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T20:05:31.251280Z",
     "start_time": "2019-05-21T14:39:39.245739Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "colab_type": "code",
    "id": "qzBetvNr5cJ9",
    "outputId": "8a20e0dd-44d8-4128-fb91-905a063ff6b5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "stacker.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T13:14:42.409372Z",
     "start_time": "2019-05-21T13:13:54.696762Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "dii73TD65cJ_",
    "outputId": "05e621c0-c10b-4566-c9bc-b2298f28ff8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stacker acc on train: 0.3938895553577648\n"
     ]
    }
   ],
   "source": [
    "print('stacker acc on train:', stacker.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T14:12:45.167657Z",
     "start_time": "2019-05-21T13:14:42.422337Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "ugctR1ni5cKD",
    "outputId": "256205e8-4275-455e-c499-99d0ef7f5988"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stacker cross-val acc  on train: 0.37537234507695616\n"
     ]
    }
   ],
   "source": [
    "cv = 5\n",
    "print('stacker cross-val acc  on train:', np.mean(cross_val_score(stacker.metaclassifier, X_train, y_train, cv=cv)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T14:12:51.612820Z",
     "start_time": "2019-05-21T14:12:45.172645Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "FGfDttjH5cKG",
    "outputId": "b1d11107-dd22-4480-b688-36b89acb603f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stacker acc on dev: 0.31570907921534996\n"
     ]
    }
   ],
   "source": [
    "print('stacker acc on dev:', stacker.score(X_dev,y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T13:07:12.300370Z",
     "start_time": "2019-05-21T13:07:12.280422Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "jBjMaFJs5cKJ",
    "outputId": "70cba3e5-ff1c-4762-cebe-12bb4eac88f3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='cosine',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=3, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = KNeighborsClassifier(n_neighbors=3, algorithm='auto', metric = \"cosine\")\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T13:07:21.805816Z",
     "start_time": "2019-05-21T13:07:21.525565Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "tnAmagp85cKL",
    "outputId": "6bd88b06-29a7-4e52-e0ae-0e0b2ca4ef14"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2890342420861846"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cross_val_score(model, X_dev, y_dev, cv=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T09:51:56.374773Z",
     "start_time": "2019-05-21T09:51:36.068625Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "id": "cVoQwjnu5cKP",
    "outputId": "e7c80581-c123-4fbb-de39-53497151ef22"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 46,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model =  LogisticRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T09:53:22.920988Z",
     "start_time": "2019-05-21T09:52:58.787901Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "8NkA_fCf5cKR",
    "outputId": "60dc3438-c8a7-4fd8-c7f1-d4d99668dec1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 48,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_MNB = MultinomialNB()\n",
    "model_MNB.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T09:53:28.846271Z",
     "start_time": "2019-05-21T09:53:22.953900Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "V6eaLKDr5cKT"
   },
   "outputs": [],
   "source": [
    "predict_labels = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-21T14:39:45.091Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "By9c0Ot_5cKX"
   },
   "outputs": [],
   "source": [
    "predict_labels = model_MNB.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-21T14:39:46.041Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "P6cfZQIa5cKZ"
   },
   "outputs": [],
   "source": [
    "def test_prediction_to_submit_file(test_lables, filepath):\n",
    "    with open(filepath, 'w',  newline='') as writeFile:\n",
    "        writer = csv.writer(writeFile)\n",
    "        writer.writerows([[\"Id\", \"Class\"]])\n",
    "        for i in list(range(len(test_lables))):\n",
    "            index = '3' + str(i+1)\n",
    "            writer.writerows([[index, test_lables[i]]])\n",
    "        writeFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-21T14:39:47.496Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "5aefXHXr5cKb"
   },
   "outputs": [],
   "source": [
    "test_prediction_to_submit_file(predict_labels, \"predicted_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aZCk0jjE5cKd"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "aF3Nd94Q5cJj"
   ],
   "name": "ass2 WLH with DNN.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
