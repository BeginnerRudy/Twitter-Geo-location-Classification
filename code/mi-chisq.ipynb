{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T23:07:49.175895Z",
     "start_time": "2019-05-21T23:07:47.837462Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\91260\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\91260\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import time\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords  \n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read raw text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T23:07:49.186854Z",
     "start_time": "2019-05-21T23:07:49.178874Z"
    }
   },
   "outputs": [],
   "source": [
    "train_raw_filepath =  \"./Short-Text-Location-Prediction/2019S1-proj2-data_dos/train-raw.tsv\"\n",
    "dev_raw_filepath =  \"./Short-Text-Location-Prediction/2019S1-proj2-data_dos/dev-raw.tsv\"\n",
    "test_raw_filepath =  \"./Short-Text-Location-Prediction/2019S1-proj2-data_dos/test-raw.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T23:07:49.201814Z",
     "start_time": "2019-05-21T23:07:49.189846Z"
    }
   },
   "outputs": [],
   "source": [
    "# read tsv file into a 2D array\n",
    "def read_tsv(filepath):\n",
    "    label, text = [], []\n",
    "    with open(filepath) as raw:\n",
    "        reader = csv.reader(raw, delimiter=\"\\t\", quoting = csv.QUOTE_NONE)\n",
    "        for row in reader:\n",
    "            label.append(row[1])\n",
    "            text.append(row[-1])\n",
    "    return text, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T23:07:49.243703Z",
     "start_time": "2019-05-21T23:07:49.233729Z"
    }
   },
   "outputs": [],
   "source": [
    "def is_all_zero(instance):\n",
    "    for attr in instance:\n",
    "        if int(attr) != 0:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def remove_all_zero_instance(X, y):\n",
    "    new_X, new_y = [], []\n",
    "    for i in range(len(y)):\n",
    "        instance = X[i, :].toarray().sum(axis = 0)\n",
    "        if is_all_zero(instance) is False:\n",
    "            new_X.append(X[i])\n",
    "            new_y.append(y[i])\n",
    "    return new_X, new_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Pre-filtering  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T23:07:49.594765Z",
     "start_time": "2019-05-21T23:07:49.578806Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_URL(texts):\n",
    "    for i in list(range(len(texts))):\n",
    "        texts[i] = re.sub(r\"http\\S+\", \"\", texts[i])\n",
    "        \n",
    "def remove_metion(texts):\n",
    "    for i in list(range(len(texts))):\n",
    "        texts[i] = re.sub(r\"@\\S+\", \"\", texts[i])\n",
    "        texts[i] = re.sub(r\"@\", \"\", texts[i])\n",
    "        \n",
    "def remove_hash(texts):\n",
    "    for i in list(range(len(texts))):\n",
    "        texts[i] = re.sub(r\"#\\S+\", \"\", texts[i])\n",
    "        texts[i] = re.sub(r\"#\", \"\", texts[i])\n",
    "\n",
    "def remove_unicode(texts):\n",
    "    for i in list(range(len(texts))):\n",
    "        texts[i] = re.sub(r\"\\\\uS+\", \"\", texts[i])\n",
    "        texts[i] = re.sub(r\"\\\\\", \"\", texts[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WHL calculating "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T23:07:49.923885Z",
     "start_time": "2019-05-21T23:07:49.912914Z"
    }
   },
   "outputs": [],
   "source": [
    "class Word_Counter:\n",
    "    # Initializer / Instance Attributes\n",
    "    def __init__(self, size):\n",
    "        self.total_word_count = [0]*size\n",
    "        self.Melbourne_word_count = [0]*size\n",
    "        self.Sydney_word_count = [0]*size \n",
    "        self.Perth_word_count = [0]*size        \n",
    "        self.Brisbane_word_count = [0]*size        \n",
    "class WLH_lists:\n",
    "    # Initializer / Instance Attributes\n",
    "    def __init__(self):\n",
    "        self.Melbourne_WLH_list = []\n",
    "        self.Sydney_WLH_list = [] \n",
    "        self.Perth_WLH_list = []        \n",
    "        self.Brisbane_WLH_list = []        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T23:07:50.095429Z",
     "start_time": "2019-05-21T23:07:50.079469Z"
    }
   },
   "outputs": [],
   "source": [
    "def word_frequency_counter(counts, lables):\n",
    "    # create a list to store count\n",
    "    num_words = counts.shape[1]\n",
    "    counter = Word_Counter(num_words)\n",
    "    \n",
    "    for i in list(range(len(lables))):\n",
    "        instance = counts[i, :]\n",
    "        if (lables[i] == \"Melbourne\"):\n",
    "            counter.Melbourne_word_count += instance.toarray().sum(axis=0)\n",
    "        elif (lables[i] == \"Sydney\"):\n",
    "            counter.Sydney_word_count  += instance.toarray().sum(axis=0)\n",
    "        elif (lables[i] == \"Perth\"):\n",
    "            counter.Perth_word_count += instance.toarray().sum(axis=0)\n",
    "        elif (lables[i] == \"Brisbane\"):\n",
    "            counter.Brisbane_word_count += instance.toarray().sum(axis=0)\n",
    "    \n",
    "    counter.total_word_count = counter.Melbourne_word_count + \\\n",
    "                                counter.Sydney_word_count +\\\n",
    "                                counter.Perth_word_count +\\\n",
    "                                counter.Brisbane_word_count\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T23:07:50.408589Z",
     "start_time": "2019-05-21T23:07:50.401609Z"
    }
   },
   "outputs": [],
   "source": [
    "def occur_in_n_state(WHL_list):\n",
    "    count = 0\n",
    "    for WHL in WHL_list:\n",
    "        if WHL > 0:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T23:07:50.609053Z",
     "start_time": "2019-05-21T23:07:50.573148Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_WHL_by_state(word_counter, vectoriser):\n",
    "    WLH_list_occur_more_than_one_state = []\n",
    "    WLH_list_one_state = []\n",
    "    \n",
    "    #  total number of word used for each word counter\n",
    "    Melbourne_total_word_num = word_counter.Melbourne_word_count.sum()\n",
    "    Brisbane_total_word_num = word_counter.Brisbane_word_count.sum()\n",
    "    Sydney_total_word_num = word_counter.Sydney_word_count.sum()\n",
    "    Perth_total_word_num = word_counter.Perth_word_count.sum()\n",
    "    all_state_total_word_num = word_counter.total_word_count.sum()\n",
    "\n",
    "    for i in list(range(len(word_counter.total_word_count))):    \n",
    "    # for i in list(range(100)):\n",
    "        curr_word_list = [] # [WHL, word, frequency,  state]\n",
    "        curr_word = vectoriser.get_feature_names()[i]\n",
    "\n",
    "        curr_word_total_prob = word_counter.total_word_count[i]/all_state_total_word_num\n",
    "\n",
    "        WHL_Mel = (word_counter.Melbourne_word_count[i]/Melbourne_total_word_num)/curr_word_total_prob\n",
    "        WHL_Syd = (word_counter.Sydney_word_count[i]/Sydney_total_word_num)/curr_word_total_prob\n",
    "        WHL_Per = (word_counter.Perth_word_count[i]/Perth_total_word_num)/curr_word_total_prob\n",
    "        WHL_Bri = (word_counter.Brisbane_word_count[i]/Brisbane_total_word_num)/curr_word_total_prob\n",
    "\n",
    "        WHL_list = [WHL_Mel, WHL_Bri, WHL_Per, WHL_Syd]\n",
    "        if (occur_in_n_state(WHL_list) > 1):\n",
    "            max_WHL = max(WHL_list)\n",
    "\n",
    "            state = [] # (stateName, wordStateCount, wordTotalCount)\n",
    "            if (WHL_Mel == max_WHL):\n",
    "                state.append([\"Melbourne\", word_counter.Melbourne_word_count[i], word_counter.Melbourne_word_count[i]/word_counter.total_word_count[i]])\n",
    "            if (WHL_Bri == max_WHL):\n",
    "                state.append([\"Brisbane\", word_counter.Brisbane_word_count[i], word_counter.Brisbane_word_count[i]/word_counter.total_word_count[i]])\n",
    "            if (WHL_Per == max_WHL):\n",
    "                state.append([\"Perth\", word_counter.Perth_word_count[i], word_counter.Perth_word_count[i]/word_counter.total_word_count[i]])\n",
    "            if (WHL_Syd == max_WHL):\n",
    "                state.append([\"Sydney\", word_counter.Sydney_word_count[i], word_counter.Sydney_word_count[i]/word_counter.total_word_count[i]])\n",
    "            curr_word_list.append(max_WHL)\n",
    "            curr_word_list.append(i)\n",
    "            curr_word_list.append(curr_word)\n",
    "            curr_word_list.append(state)\n",
    "            WLH_list_occur_more_than_one_state.append(curr_word_list)\n",
    "        else:\n",
    "            max_WHL = max(WHL_list)\n",
    "\n",
    "            state = [] # (stateName, wordStateCount, wordTotalCount)\n",
    "            if (WHL_Mel == max_WHL):\n",
    "                state.append([\"Melbourne\", word_counter.Melbourne_word_count[i], word_counter.Melbourne_word_count[i]/word_counter.total_word_count[i]])\n",
    "            elif (WHL_Bri == max_WHL):\n",
    "                state.append([\"Brisbane\", word_counter.Brisbane_word_count[i], word_counter.Brisbane_word_count[i]/word_counter.total_word_count[i]])\n",
    "            elif (WHL_Per == max_WHL):\n",
    "                state.append([\"Perth\", word_counter.Perth_word_count[i], word_counter.Perth_word_count[i]/word_counter.total_word_count[i]])\n",
    "            elif (WHL_Syd == max_WHL):\n",
    "                state.append([\"Sydney\", word_counter.Sydney_word_count[i], word_counter.Sydney_word_count[i]/word_counter.total_word_count[i]])\n",
    "            curr_word_list.append(max_WHL)\n",
    "            curr_word_list.append(i)\n",
    "            curr_word_list.append(curr_word)\n",
    "            curr_word_list.append(state)\n",
    "            WLH_list_one_state.append(curr_word_list)\n",
    "    return WLH_list_occur_more_than_one_state, WLH_list_one_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T23:07:50.664903Z",
     "start_time": "2019-05-21T23:07:50.653933Z"
    }
   },
   "outputs": [],
   "source": [
    "def groupby_state(WLHs):\n",
    "    WLH_groupby_state = WLH_lists()\n",
    "        # Initializer / Instance Attributes\n",
    "\n",
    "        \n",
    "    for WLH in WLHs:\n",
    "        state = WLH[3][0][0]\n",
    "        if state == \"Melbourne\":\n",
    "            WLH_groupby_state.Melbourne_WLH_list.append(WLH)\n",
    "        elif state == \"Sydney\":\n",
    "            WLH_groupby_state.Sydney_WLH_list.append(WLH)\n",
    "        elif state == \"Perth\":\n",
    "            WLH_groupby_state.Perth_WLH_list.append(WLH)            \n",
    "        elif state == \"Brisbane\":\n",
    "            WLH_groupby_state.Brisbane_WLH_list.append(WLH)\n",
    "    sorted(WLH_groupby_state.Melbourne_WLH_list, key=lambda x: x[0], reverse=True)\n",
    "    sorted(WLH_groupby_state.Sydney_WLH_list, key=lambda x: x[0], reverse=True)\n",
    "    sorted(WLH_groupby_state.Perth_WLH_list, key=lambda x: x[0], reverse=True)\n",
    "    sorted(WLH_groupby_state.Brisbane_WLH_list, key=lambda x: x[0], reverse=True)\n",
    "    return WLH_groupby_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Top k WHL for each state "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T23:07:51.042896Z",
     "start_time": "2019-05-21T23:07:51.030925Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_top_k_index(state_WLH_list, k):\n",
    "    indice = []\n",
    "    WLH_top_k = (sorted(state_WLH_list, key=lambda x: x[0], reverse=True))[:k]\n",
    "    \n",
    "    for WLH in WLH_top_k:\n",
    "        indice.append(WLH[1])\n",
    "    return indice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T23:07:51.265300Z",
     "start_time": "2019-05-21T23:07:51.258319Z"
    }
   },
   "outputs": [],
   "source": [
    "# select cloumn from X_train by top 10 index for each state\n",
    "def extract_column_by_index(matrix, indice, vectoriser):\n",
    "    selected_words = []\n",
    "    for index in indice:\n",
    "        selected_words.append(vectoriser.get_feature_names()[index])\n",
    "    \n",
    "    result = []\n",
    "    length = matrix.shape[0]\n",
    "    for i in list(range(length)):\n",
    "        row = matrix[i, :].toarray().sum(axis = 0)\n",
    "        extract = []\n",
    "        for index in indice:\n",
    "            extract.append(row[index])\n",
    "        result.append(extract)\n",
    "    return result, selected_words\n",
    "# feature engineering done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T23:07:51.661244Z",
     "start_time": "2019-05-21T23:07:51.643289Z"
    }
   },
   "outputs": [],
   "source": [
    "# This function take the filepath of the text and return a word2vec object\n",
    "def generate_word2vec(filepath):\n",
    "    # read the file into two parts, text and its location\n",
    "    X_raw, y_train = read_tsv(filepath)\n",
    "\n",
    "    # remove URL, hash tag and metion\n",
    "    remove_URL(X_raw)\n",
    "    remove_hash(X_raw)\n",
    "    remove_metion(X_raw)\n",
    "\n",
    "    # remian only letters and numbers\n",
    "    processed_tweets = [re.sub('[^a-zA-Z-0-9]', ' ', tweet) for tweet in X_raw]\n",
    "\n",
    "    # make it into one string and out it into a list => the input format\n",
    "    processed_tweets_in_one = \"\"\n",
    "    for tweet in processed_tweets:\n",
    "        processed_tweets_in_one += tweet\n",
    "    processed_tweets_in_one = processed_tweets_in_one.lower()\n",
    "    processed_tweets_in_one = [processed_tweets_in_one]\n",
    "\n",
    "    # extract each word and make then into 1 list\n",
    "    all_words = [nltk.word_tokenize(tweet) for tweet in processed_tweets_in_one]\n",
    "\n",
    "    # Removing Stop Words\n",
    "    for i in range(len(all_words)):  \n",
    "        all_words[i] = [w for w in all_words[i] if w not in stopwords.words('english')]\n",
    "\n",
    "    # Create word2vec    \n",
    "    return Word2Vec(all_words, min_count=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put all together => Feature Engineering Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T23:07:52.327460Z",
     "start_time": "2019-05-21T23:07:52.314495Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocessing_test(filepath, selected_words):\n",
    "    # read the file into two parts, text and its location\n",
    "    X_raw, y = read_tsv(filepath)\n",
    "    \n",
    "    # remove URL, hash tag and metion\n",
    "    remove_URL(X_raw)\n",
    "#     remove_hash(X_raw)\n",
    "#     remove_metion(X_raw)\n",
    "    \n",
    "    # initilaze vectoriser\n",
    "    vectoriser = CountVectorizer(stop_words=\"english\", vocabulary=selected_words)\n",
    "    X_sparse = vectoriser.fit_transform(X_raw)\n",
    "    \n",
    "    \n",
    "    # make sparse matrix into 2D list\n",
    "    X = []\n",
    "    for i in list(range(len(y))):\n",
    "        X.append(X_sparse[i, :].toarray().sum(axis = 0))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T23:07:52.713429Z",
     "start_time": "2019-05-21T23:07:52.701460Z"
    }
   },
   "outputs": [],
   "source": [
    "#This function takes in \n",
    "#@param: filepath => The filepath of the corpus\n",
    "def preprocessing_train(filepath):\n",
    "    # read the file into two parts, text and its location\n",
    "    X_raw, y_train = read_tsv(filepath)\n",
    "    \n",
    "    # remove URL, hash tag and metion\n",
    "    remove_URL(X_raw)\n",
    "    remove_hash(X_raw)\n",
    "    remove_metion(X_raw)\n",
    "    \n",
    "    # initilaze vectoriser\n",
    "    vectoriser = CountVectorizer(stop_words=\"english\", min_df = 15 )\n",
    "    X_train = vectoriser.fit_transform(X_raw)\n",
    "    \n",
    "    # calculating WLH\n",
    "    word_counter = word_frequency_counter(X_train, y_train)\n",
    "\n",
    "    # count frequency for each word by state\n",
    "    WLH_list_occur_more_than_one_state, WLH_list_one_state = calculate_WHL_by_state(word_counter, vectoriser)\n",
    "\n",
    "    # group WLH by state\n",
    "    WLH_lists_gourpby_state = groupby_state(WLH_list_occur_more_than_one_state)\n",
    "    \n",
    "    # select top k WHL\n",
    "    num_features = len(vectoriser.get_feature_names())\n",
    "    percent = 0.14\n",
    "    k = percent*num_features/4\n",
    "    k = int(k)\n",
    "    k = 300\n",
    "    \n",
    "    # extract top 10 WLH index for each state\n",
    "    top_k_indice_for_each_state = []\n",
    "    top_k_indice_for_each_state.extend(extract_top_k_index(WLH_lists_gourpby_state.Brisbane_WLH_list, k))\n",
    "    top_k_indice_for_each_state.extend(extract_top_k_index(WLH_lists_gourpby_state.Melbourne_WLH_list, k))\n",
    "    top_k_indice_for_each_state.extend(extract_top_k_index(WLH_lists_gourpby_state.Perth_WLH_list, k))\n",
    "    top_k_indice_for_each_state.extend(extract_top_k_index(WLH_lists_gourpby_state.Sydney_WLH_list, k))\n",
    "    \n",
    "    # extract columns\n",
    "    new_train, selected_words = extract_column_by_index(X_train, top_k_indice_for_each_state, vectoriser)\n",
    "    return new_train, y_train, selected_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T23:07:53.265952Z",
     "start_time": "2019-05-21T23:07:53.255979Z"
    }
   },
   "outputs": [],
   "source": [
    "#This function takes in \n",
    "#@param: filepath => The filepath of the corpus\n",
    "def get_top_k_index(filepath):\n",
    "    # read the file into two parts, text and its location\n",
    "    X_raw, y_train = read_tsv(filepath)\n",
    "    \n",
    "    # remove URL, hash tag and metion\n",
    "    remove_URL(X_raw)\n",
    "    remove_hash(X_raw)\n",
    "    remove_metion(X_raw)\n",
    "    \n",
    "    # initilaze vectoriser\n",
    "    vectoriser = CountVectorizer(stop_words=\"english\", min_df = 15 )\n",
    "    X_train = vectoriser.fit_transform(X_raw)\n",
    "    \n",
    "    # calculating WLH\n",
    "    word_counter = word_frequency_counter(X_train, y_train)\n",
    "\n",
    "    # count frequency for each word by state\n",
    "    WLH_list_occur_more_than_one_state, WLH_list_one_state = calculate_WHL_by_state(word_counter, vectoriser)\n",
    "\n",
    "    # group WLH by state\n",
    "    WLH_lists_gourpby_state = groupby_state(WLH_list_occur_more_than_one_state)\n",
    "    \n",
    "    # select top k WHL\n",
    "    num_features = len(vectoriser.get_feature_names())\n",
    "    percent = 0.14\n",
    "    k = percent*num_features/4\n",
    "    k = int(k)\n",
    "    k = 20\n",
    "    \n",
    "    # extract top 10 WLH index for each state\n",
    "    top_k_indice_for_each_state = []\n",
    "    top_k_indice_for_each_state.extend(extract_top_k_index(WLH_lists_gourpby_state.Brisbane_WLH_list, k))\n",
    "    top_k_indice_for_each_state.extend(extract_top_k_index(WLH_lists_gourpby_state.Melbourne_WLH_list, k))\n",
    "    top_k_indice_for_each_state.extend(extract_top_k_index(WLH_lists_gourpby_state.Perth_WLH_list, k))\n",
    "    top_k_indice_for_each_state.extend(extract_top_k_index(WLH_lists_gourpby_state.Sydney_WLH_list, k))\n",
    "    \n",
    "    return top_k_indice_for_each_state, vectoriser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T23:07:54.224389Z",
     "start_time": "2019-05-21T23:07:54.219405Z"
    }
   },
   "outputs": [],
   "source": [
    "# select cloumn from X_train by top 10 index for each state\n",
    "def get_selected_words(indice, vectoriser):\n",
    "    return [vectoriser.get_feature_names()[index] for index in indice]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T23:07:55.169864Z",
     "start_time": "2019-05-21T23:07:55.155900Z"
    }
   },
   "outputs": [],
   "source": [
    "def print_0_percet(X, y):\n",
    "    num_instance = X.shape[0]\n",
    "    X_no_zero, y_no_zero = remove_all_zero_instance(X, y)\n",
    "\n",
    "    num_all_zero_instance = num_instance - len(X_no_zero.shape[0])\n",
    "    print(\"The size of total dataset is %d\"%num_instance)\n",
    "    print(\"The size of all zero instance in this dataset is %d\"%num_all_zero_instance)\n",
    "    print(\"The ratio of all zero instances is %f \"%(num_all_zero_instance/num_instance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution of feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-21T23:08:03.267Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, y_train, selected_words = preprocessing_train(train_raw_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-21T23:08:04.624Z"
    }
   },
   "outputs": [],
   "source": [
    "print_0_percet(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-19T00:58:59.498939Z",
     "start_time": "2019-05-19T00:58:45.159216Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of total dataset is 37316\n",
      "The size of all zero instance in this dataset is 27187\n",
      "The ratio of all zero instances is 0.728561 \n"
     ]
    }
   ],
   "source": [
    "X_dev, y_dev = preprocessing_test(dev_raw_filepath, selected_words)\n",
    "print_0_percet(X_dev, y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-19T00:59:40.502785Z",
     "start_time": "2019-05-19T00:58:59.501927Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of total dataset is 108148\n",
      "The size of all zero instance in this dataset is 83376\n",
      "The ratio of all zero instances is 0.770944 \n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = preprocessing_test(test_raw_filepath, selected_words)\n",
    "print_0_percet(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MI and ChiSq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T05:15:53.524667Z",
     "start_time": "2019-05-21T05:15:52.886939Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_raw, y_train = read_tsv(train_raw_filepath)\n",
    "X_dev_raw, y_dev = read_tsv(dev_raw_filepath)\n",
    "X_test_raw, y_test = read_tsv(test_raw_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T05:15:55.398557Z",
     "start_time": "2019-05-21T05:15:54.701303Z"
    }
   },
   "outputs": [],
   "source": [
    "remove_URL(X_train_raw)\n",
    "remove_hash(X_train_raw)\n",
    "remove_metion(X_train_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T05:15:56.346614Z",
     "start_time": "2019-05-21T05:15:55.566699Z"
    }
   },
   "outputs": [],
   "source": [
    "remove_URL(X_test_raw)\n",
    "remove_hash(X_test_raw)\n",
    "remove_metion(X_test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T05:16:00.403201Z",
     "start_time": "2019-05-21T05:15:58.637848Z"
    }
   },
   "outputs": [],
   "source": [
    "vectoriser = CountVectorizer(stop_words='english')\n",
    "X_train = vectoriser.fit_transform(X_train_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T05:16:02.248148Z",
     "start_time": "2019-05-21T05:16:00.942636Z"
    }
   },
   "outputs": [],
   "source": [
    "X_test = vectoriser.transform(X_test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T05:16:02.670399Z",
     "start_time": "2019-05-21T05:16:02.665381Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(108148, 69932)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T05:16:05.963832Z",
     "start_time": "2019-05-21T05:16:05.959842Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "mi = SelectKBest(score_func=mutual_info_classif, k=10)\n",
    "# X_train_mi = mi.fit_transform(X_train,y_train)\n",
    "# X_test_mi = mi.transform(X_test)\n",
    "\n",
    "# print(X_test_mi.shape, X_train_mi.shape)\n",
    "\n",
    "# for feat_num in mi.get_support(indices=True):\n",
    "#     print(vectoriser.get_feature_names()[feat_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T06:05:45.317331Z",
     "start_time": "2019-05-21T05:17:05.173826Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\software\\anaconda\\lib\\site-packages\\sklearn\\metrics\\cluster\\supervised.py:605: RuntimeWarning: invalid value encountered in log\n",
      "  log_outer = -np.log(outer) + log(pi.sum()) + log(pj.sum())\n"
     ]
    }
   ],
   "source": [
    "X_train_mi = mi.fit_transform(X_train ,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T06:09:27.785692Z",
     "start_time": "2019-05-21T06:09:27.780700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(103364, 10)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_mi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T06:09:30.557945Z",
     "start_time": "2019-05-21T06:09:29.874583Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u2026\n",
      "zwaanswijk\n",
      "zx\n",
      "zyrtec\n",
      "zz\n",
      "zzz\n",
      "zzzigarrr\n",
      "zzzz\n",
      "zzzzz\n",
      "zzzzzzz\n"
     ]
    }
   ],
   "source": [
    "for feat_num in mi.get_support(indices=True):\n",
    "    print(vectoriser.get_feature_names()[feat_num])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-19T00:59:40.517744Z",
     "start_time": "2019-05-19T00:59:40.504779Z"
    }
   },
   "outputs": [],
   "source": [
    "models = [DummyClassifier(strategy='most_frequent'),\n",
    "          GaussianNB(),\n",
    "          MultinomialNB(),\n",
    "          LogisticRegression()]\n",
    "titles = ['Zero-R',\n",
    "          'GNB',\n",
    "          'MNB',\n",
    "          'Logistic Regression']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-19T01:01:35.807512Z",
     "start_time": "2019-05-19T00:59:40.519739Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-R 0.25 time: 27.401124715805054\n",
      "GNB 0.2981835844959957 time: 30.704606771469116\n",
      "MNB 0.32465994139537885 time: 21.044564485549927\n",
      "Logistic Regression 0.3263209327518871 time: 27.389904022216797\n"
     ]
    }
   ],
   "source": [
    "# read csv file\n",
    "# i = 1\n",
    "# train_X, train_y = load_dataset(train_filepath[i])\n",
    "X_dev_no_0, y_dev_no_0 = remove_all_zero_instance(X_dev, y_dev)\n",
    "# dev_X, dev_y = load_dataset(dev_filepath[i])\n",
    "\n",
    "\n",
    "# try each model without feature selection\n",
    "for title, model in zip(titles, models):\n",
    "    start = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    acc = np.mean(cross_val_score(model, X_dev, y_dev, cv=10))\n",
    "#     acc = np.mean(cross_val_score(model, X_train, y_train, cv=10))\n",
    "    end = time.time()\n",
    "    t = end - start\n",
    "    print(title, acc, 'time:', t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-19T01:01:51.881587Z",
     "start_time": "2019-05-19T01:01:35.816537Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-19T01:01:59.331032Z",
     "start_time": "2019-05-19T01:01:51.884572Z"
    }
   },
   "outputs": [],
   "source": [
    "predict_labels = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-19T01:01:59.375912Z",
     "start_time": "2019-05-19T01:01:59.335021Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_prediction_to_submit_file(test_lables, filepath):\n",
    "    with open(filepath, 'w',  newline='') as writeFile:\n",
    "        writer = csv.writer(writeFile)\n",
    "        writer.writerows([[\"Id\", \"Class\"]])\n",
    "        for i in list(range(len(test_lables))):\n",
    "            index = '3' + str(i+1)\n",
    "            writer.writerows([[index, test_lables[i]]])\n",
    "        writeFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-19T01:01:59.809752Z",
     "start_time": "2019-05-19T01:01:59.381897Z"
    }
   },
   "outputs": [],
   "source": [
    "test_prediction_to_submit_file(predict_labels, \"predicted_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-19T01:01:59.823715Z",
     "start_time": "2019-05-19T01:01:59.812745Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sydney'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "327.638px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
