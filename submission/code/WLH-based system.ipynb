{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T05:11:24.335843Z",
     "start_time": "2019-05-25T05:11:07.896794Z"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import time\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords  \n",
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "import sys\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read raw text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T05:11:24.352791Z",
     "start_time": "2019-05-25T05:11:24.341821Z"
    }
   },
   "outputs": [],
   "source": [
    "train_raw_filepath =  \"./2019S1-proj2-data_dos/train-raw.tsv\"\n",
    "dev_raw_filepath =  \"./2019S1-proj2-data_dos/dev-raw.tsv\"\n",
    "test_raw_filepath =  \"./2019S1-proj2-data_dos/test-raw.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T05:11:24.371740Z",
     "start_time": "2019-05-25T05:11:24.357780Z"
    }
   },
   "outputs": [],
   "source": [
    "# read tsv file into a 2D array\n",
    "def read_tsv(filepath):\n",
    "    label, text = [], []\n",
    "    with open(filepath) as raw:\n",
    "        reader = csv.reader(raw, delimiter=\"\\t\", quoting = csv.QUOTE_NONE)\n",
    "        for row in reader:\n",
    "            label.append(row[1])\n",
    "            text.append(row[-1])\n",
    "    return text, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T05:11:24.395677Z",
     "start_time": "2019-05-25T05:11:24.377726Z"
    }
   },
   "outputs": [],
   "source": [
    "def is_all_zero(instance):\n",
    "    for attr in instance:\n",
    "        if float(attr) != 0.0:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def remove_all_zero_instance(X, y):\n",
    "    new_X, new_y = [], []\n",
    "    for i in range(len(y)):\n",
    "        instance = X[i]\n",
    "        if is_all_zero(instance) is False:\n",
    "            new_X.append(X[i])\n",
    "            new_y.append(y[i])\n",
    "    return new_X, new_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Text Pre-filtering  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T05:11:25.514683Z",
     "start_time": "2019-05-25T05:11:24.401663Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## WHL calculating "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T05:11:25.526654Z",
     "start_time": "2019-05-25T05:11:25.517677Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Word_Counter:\n",
    "    # Initializer / Instance Attributes\n",
    "    def __init__(self, size):\n",
    "        self.total_word_count = [0]*size\n",
    "        self.Melbourne_word_count = [0]*size\n",
    "        self.Sydney_word_count = [0]*size \n",
    "        self.Perth_word_count = [0]*size        \n",
    "        self.Brisbane_word_count = [0]*size        \n",
    "class WLH_lists:\n",
    "    # Initializer / Instance Attributes\n",
    "    def __init__(self):\n",
    "        self.Melbourne_WLH_list = []\n",
    "        self.Sydney_WLH_list = [] \n",
    "        self.Perth_WLH_list = []        \n",
    "        self.Brisbane_WLH_list = []        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T05:11:25.551588Z",
     "start_time": "2019-05-25T05:11:25.534644Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def word_frequency_counter(counts, lables):\n",
    "    # create a list to store count\n",
    "    num_words = counts.shape[1]\n",
    "    counter = Word_Counter(num_words)\n",
    "    \n",
    "    for i in list(range(len(lables))):\n",
    "        instance = counts[i, :]\n",
    "        if (lables[i] == \"Melbourne\"):\n",
    "            counter.Melbourne_word_count += instance.toarray().sum(axis=0)\n",
    "        elif (lables[i] == \"Sydney\"):\n",
    "            counter.Sydney_word_count  += instance.toarray().sum(axis=0)\n",
    "        elif (lables[i] == \"Perth\"):\n",
    "            counter.Perth_word_count += instance.toarray().sum(axis=0)\n",
    "        elif (lables[i] == \"Brisbane\"):\n",
    "            counter.Brisbane_word_count += instance.toarray().sum(axis=0)\n",
    "    \n",
    "    counter.total_word_count = counter.Melbourne_word_count + \\\n",
    "                                counter.Sydney_word_count +\\\n",
    "                                counter.Perth_word_count +\\\n",
    "                                counter.Brisbane_word_count\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T05:11:25.578514Z",
     "start_time": "2019-05-25T05:11:25.557569Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def calculate_WHL_by_state(word_counter, vectoriser):\n",
    "    WLH_groupby_state = WLH_lists()\n",
    "    #  total number of word used for each word counter\n",
    "    Melbourne_total_word_num = word_counter.Melbourne_word_count.sum()\n",
    "    Brisbane_total_word_num = word_counter.Brisbane_word_count.sum()\n",
    "    Sydney_total_word_num = word_counter.Sydney_word_count.sum()\n",
    "    Perth_total_word_num = word_counter.Perth_word_count.sum()\n",
    "    all_state_total_word_num = word_counter.total_word_count.sum()\n",
    "\n",
    "    for i in list(range(len(word_counter.total_word_count))):    \n",
    "    # for i in list(range(100)):\n",
    "        curr_word_list = [] # [WHL, word, state]\n",
    "#         curr_word = vectoriser.get_feature_names()[i]\n",
    "\n",
    "        curr_word_total_prob = word_counter.total_word_count[i]/all_state_total_word_num\n",
    "\n",
    "        WHL_Mel = (word_counter.Melbourne_word_count[i]/Melbourne_total_word_num)/curr_word_total_prob\n",
    "        WHL_Syd = (word_counter.Sydney_word_count[i]/Sydney_total_word_num)/curr_word_total_prob\n",
    "        WHL_Per = (word_counter.Perth_word_count[i]/Perth_total_word_num)/curr_word_total_prob\n",
    "        WHL_Bri = (word_counter.Brisbane_word_count[i]/Brisbane_total_word_num)/curr_word_total_prob\n",
    "\n",
    "        WHL_list = [WHL_Mel, WHL_Bri, WHL_Per, WHL_Syd]\n",
    "        max_WHL = max(WHL_list)\n",
    "        \n",
    "        curr_word_list.append(max_WHL)\n",
    "        curr_word_list.append(i)\n",
    "        \n",
    "        if (WHL_Mel == max_WHL):\n",
    "            WLH_groupby_state.Melbourne_WLH_list.append(curr_word_list)\n",
    "        elif (WHL_Bri == max_WHL):\n",
    "            WLH_groupby_state.Brisbane_WLH_list.append(curr_word_list)\n",
    "        elif (WHL_Per == max_WHL):\n",
    "            WLH_groupby_state.Perth_WLH_list.append(curr_word_list) \n",
    "        elif (WHL_Syd == max_WHL):\n",
    "            WLH_groupby_state.Sydney_WLH_list.append(curr_word_list)\n",
    "            \n",
    "    sorted(WLH_groupby_state.Melbourne_WLH_list, key=lambda x: x[0], reverse=True)\n",
    "    sorted(WLH_groupby_state.Sydney_WLH_list, key=lambda x: x[0], reverse=True)\n",
    "    sorted(WLH_groupby_state.Perth_WLH_list, key=lambda x: x[0], reverse=True)\n",
    "    sorted(WLH_groupby_state.Brisbane_WLH_list, key=lambda x: x[0], reverse=True)\n",
    "    return WLH_groupby_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T05:11:25.606439Z",
     "start_time": "2019-05-25T05:11:25.584499Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def groupby_state(WLHs):\n",
    "    WLH_groupby_state = WLH_lists()\n",
    "        # Initializer / Instance Attributes\n",
    "\n",
    "        \n",
    "    for WLH in WLHs:\n",
    "        state = WLH[3][0][0]\n",
    "        if state == \"Melbourne\":\n",
    "            WLH_groupby_state.Melbourne_WLH_list.append(WLH)\n",
    "        elif state == \"Sydney\":\n",
    "            WLH_groupby_state.Sydney_WLH_list.append(WLH)\n",
    "        elif state == \"Perth\":\n",
    "            WLH_groupby_state.Perth_WLH_list.append(WLH)            \n",
    "        elif state == \"Brisbane\":\n",
    "            WLH_groupby_state.Brisbane_WLH_list.append(WLH)\n",
    "    sorted(WLH_groupby_state.Melbourne_WLH_list, key=lambda x: x[0], reverse=True)\n",
    "    sorted(WLH_groupby_state.Sydney_WLH_list, key=lambda x: x[0], reverse=True)\n",
    "    sorted(WLH_groupby_state.Perth_WLH_list, key=lambda x: x[0], reverse=True)\n",
    "    sorted(WLH_groupby_state.Brisbane_WLH_list, key=lambda x: x[0], reverse=True)\n",
    "    return WLH_groupby_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T05:11:25.628380Z",
     "start_time": "2019-05-25T05:11:25.610430Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def occur_in_n_state(WHL_list):\n",
    "    count = 0\n",
    "    for WHL in WHL_list:\n",
    "        if WHL > 0:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T09:07:09.018400Z",
     "start_time": "2019-05-21T09:07:09.014410Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "# Extracting Top k WHL for each state "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T05:11:25.660307Z",
     "start_time": "2019-05-25T05:11:25.634364Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def extract_top_k_index(state_WLH_list, k):\n",
    "    indice = []\n",
    "    WLH_top_k = state_WLH_list[:k]\n",
    "    \n",
    "    for WLH in WLH_top_k:\n",
    "        indice.append(WLH[1])\n",
    "    return indice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T05:11:25.682236Z",
     "start_time": "2019-05-25T05:11:25.665282Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# select cloumn from X_train by top 10 index for each state\n",
    "def extract_column_by_index(matrix, indice, vectoriser):\n",
    "    selected_words = []\n",
    "    for index in indice:\n",
    "        selected_words.append(vectoriser.get_feature_names()[index])\n",
    "    \n",
    "    result = []\n",
    "    length = matrix.shape[0]\n",
    "    for i in list(range(length)):\n",
    "        row = matrix[i, :].toarray().sum(axis = 0)\n",
    "        extract = []\n",
    "        for index in indice:\n",
    "            extract.append(row[index])\n",
    "        result.append(extract)\n",
    "    return result, selected_words\n",
    "# feature engineering done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Put all together => Feature Engineering Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T05:11:25.702183Z",
     "start_time": "2019-05-25T05:11:25.686225Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def preprocessing_test(filepath, selected_words):\n",
    "    # read the file into two parts, text and its location\n",
    "    X_raw, y = read_tsv(filepath)\n",
    "    \n",
    "    # remove URL, hash tag and metion\n",
    "    remove_URL(X_raw)\n",
    "#     remove_hash(X_raw)\n",
    "#     remove_metion(X_raw)\n",
    "    \n",
    "    # initilaze vectoriser\n",
    "    vectoriser = CountVectorizer(stop_words=\"english\", vocabulary=selected_words)\n",
    "    X_sparse = vectoriser.fit_transform(X_raw)\n",
    "    \n",
    "    \n",
    "    # make sparse matrix into 2D list\n",
    "    X = []\n",
    "    for i in list(range(len(y))):\n",
    "        X.append(X_sparse[i, :].toarray().sum(axis = 0))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T05:11:25.729110Z",
     "start_time": "2019-05-25T05:11:25.710165Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#This function takes in \n",
    "#@param: filepath => The filepath of the corpus\n",
    "def preprocessing_train(filepath, k):\n",
    "    # read the file into two parts, text and its location\n",
    "    X_raw, y_train = read_tsv(filepath)\n",
    "    \n",
    "    # remove URL, hash tag and metion\n",
    "    remove_URL(X_raw)\n",
    "    remove_hash(X_raw)\n",
    "    remove_metion(X_raw)\n",
    "    \n",
    "    # initilaze vectoriser\n",
    "    vectoriser = CountVectorizer(stop_words=\"english\", min_df = 15 )\n",
    "    X_train = vectoriser.fit_transform(X_raw)\n",
    "    \n",
    "    # calculating WLH\n",
    "    word_counter = word_frequency_counter(X_train, y_train)\n",
    "\n",
    "    # count frequency for each word by state\n",
    "    WLH_lists_gourpby_state = calculate_WHL_by_state(word_counter, vectoriser)\n",
    "\n",
    "    # group WLH by state\n",
    "#     WLH_lists_gourpby_state = groupby_state(WLH_list_occur_more_than_one_state)\n",
    "    \n",
    "    # select top k WHL\n",
    "#     k = 100\n",
    "    \n",
    "    # extract top 10 WLH index for each state\n",
    "    top_k_indice_for_each_state = []\n",
    "    top_k_indice_for_each_state.extend(extract_top_k_index(WLH_lists_gourpby_state.Brisbane_WLH_list, k))\n",
    "    top_k_indice_for_each_state.extend(extract_top_k_index(WLH_lists_gourpby_state.Melbourne_WLH_list, k))\n",
    "    top_k_indice_for_each_state.extend(extract_top_k_index(WLH_lists_gourpby_state.Perth_WLH_list, k))\n",
    "    top_k_indice_for_each_state.extend(extract_top_k_index(WLH_lists_gourpby_state.Sydney_WLH_list, k))\n",
    "    \n",
    "    # extract columns\n",
    "    new_train, selected_words = extract_column_by_index(X_train, top_k_indice_for_each_state, vectoriser)\n",
    "    return new_train, y_train, selected_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T05:11:25.752049Z",
     "start_time": "2019-05-25T05:11:25.742077Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def print_0_percet(X, y):\n",
    "    num_instance = len(X)\n",
    "    X_no_zero, y_no_zero = remove_all_zero_instance(X, y)\n",
    "\n",
    "    num_all_zero_instance = num_instance - len(X_no_zero)\n",
    "    print(\"The size of total dataset is %d\"%num_instance)\n",
    "    print(\"The size of all zero instance in this dataset is %d\"%num_all_zero_instance)\n",
    "    print(\"The ratio of all zero instances is %f \"%(num_all_zero_instance/num_instance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Main "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T23:17:31.039136Z",
     "start_time": "2019-05-24T23:17:31.026170Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_small_filepath = \"./train_raw_small.csv\"\n",
    "dev_small_filepath = \"./dev_raw_small.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T04:53:31.629208Z",
     "start_time": "2019-05-25T04:52:34.350739Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train, y_train, selected_words = preprocessing_train(train_raw_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T23:18:09.790515Z",
     "start_time": "2019-05-24T23:18:07.197449Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of total dataset is 103364\n",
      "The size of all zero instance in this dataset is 96990\n",
      "The ratio of all zero instances is 0.938334 \n"
     ]
    }
   ],
   "source": [
    "print_0_percet(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-23T05:31:26.581601Z",
     "start_time": "2019-05-23T05:31:26.574621Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T04:55:15.345658Z",
     "start_time": "2019-05-25T04:55:09.058235Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_dev, y_dev = preprocessing_test(dev_raw_filepath, selected_words)\n",
    "# print_0_percet(X_dev, y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T14:13:28.144356Z",
     "start_time": "2019-05-21T14:12:51.614815Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of total dataset is 108148\n",
      "The size of all zero instance in this dataset is 90986\n",
      "The ratio of all zero instances is 0.841310 \n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = preprocessing_test(test_raw_filepath, selected_words)\n",
    "print_0_percet(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training & Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-23T05:35:06.704909Z",
     "start_time": "2019-05-23T05:35:06.697927Z"
    }
   },
   "outputs": [],
   "source": [
    "models = [DummyClassifier(strategy='most_frequent'),\n",
    "          GaussianNB(),\n",
    "          MultinomialNB(),\n",
    "          BernoulliNB(),\n",
    "          LogisticRegression()]\n",
    "titles = ['Zero-R',\n",
    "          'GNB',\n",
    "          'MNB',\n",
    "          'BNB',\n",
    "          'Logistic Regression']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T04:52:07.121413Z",
     "start_time": "2019-05-25T04:52:07.114432Z"
    }
   },
   "outputs": [],
   "source": [
    "def put_into_dict(label, dict_):\n",
    "    if label == \"Melbourne\":\n",
    "        dict_[\"Mel\"] += 1\n",
    "    elif label == \"Sydney\":\n",
    "        dict_[\"Syd\"] += 1\n",
    "    elif label == \"Perth\":\n",
    "        dict_[\"Per\"] += 1\n",
    "    elif label == \"Brisbane\":\n",
    "        dict_[\"Bri\"] += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T04:57:32.004756Z",
     "start_time": "2019-05-25T04:57:31.994783Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Predict| Mel |Syd |Per |Bri \n",
    "Actual\n",
    "------------------------\n",
    "Mel |\n",
    "Syd |\n",
    "Per |\n",
    "Bri |\n",
    "Tot |\n",
    "'''\n",
    "def confusion_matrix_generator(predict_labels, actual_labels):\n",
    "    # It is a 2D array for store information\n",
    "    summary_dict = {}\n",
    "    summary_dict[\"Mel\"] = {\"Mel\": 0, 'Syd':0, \"Per\":0, \"Bri\": 0}\n",
    "    summary_dict[\"Syd\"] = {\"Mel\": 0, 'Syd':0, \"Per\":0, \"Bri\": 0}\n",
    "    summary_dict[\"Per\"] = {\"Mel\": 0, 'Syd':0, \"Per\":0, \"Bri\": 0}\n",
    "    summary_dict[\"Bri\"] = {\"Mel\": 0, 'Syd':0, \"Per\":0, \"Bri\": 0}\n",
    "    \n",
    "    for i in list(range(len(actual_labels))):\n",
    "        label = actual_labels[i]\n",
    "        prediction = predict_labels[i]\n",
    "        if label == \"Melbourne\":\n",
    "            put_into_dict(prediction, summary_dict[\"Mel\"])\n",
    "        elif label == \"Sydney\":\n",
    "            put_into_dict(prediction, summary_dict[\"Syd\"])\n",
    "        elif label == \"Perth\":\n",
    "            put_into_dict(prediction, summary_dict[\"Per\"])  \n",
    "        elif label == \"Brisbane\":\n",
    "            put_into_dict(prediction, summary_dict[\"Bri\"])          \n",
    "    \n",
    "    return summary_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T05:15:33.600277Z",
     "start_time": "2019-05-25T05:15:31.632538Z"
    }
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-dec162f2f3cd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mselected_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessing_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_raw_filepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mX_dev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_dev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessing_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdev_raw_filepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mselected_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-4cfecb159953>\u001b[0m in \u001b[0;36mpreprocessing_train\u001b[1;34m(filepath, k)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpreprocessing_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m# read the file into two parts, text and its location\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mX_raw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_tsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m# remove URL, hash tag and metion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-5b5ba61fd7d0>\u001b[0m in \u001b[0;36mread_tsv\u001b[1;34m(filepath)\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m             \u001b[0mlabel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m             \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "X_train, y_train, selected_words = preprocessing_train(train_raw_filepath, k)\n",
    "X_dev, y_dev = preprocessing_test(dev_raw_filepath, selected_words)\n",
    "model =  LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "prediction_dev = model.predict(X_dev)\n",
    "summary_matrix = confusion_matrix_generator(prediction_dev, y_dev)\n",
    "print(summary_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T05:15:29.065405Z",
     "start_time": "2019-05-25T05:15:29.025511Z"
    }
   },
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "This LogisticRegression instance is not fitted yet",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-18308c67b910>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprediction_dev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_dev\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\site-packages\\sklearn\\linear_model\\base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    287\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m         \"\"\"\n\u001b[1;32m--> 289\u001b[1;33m         \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    290\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\site-packages\\sklearn\\linear_model\\base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'coef_'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m             raise NotFittedError(\"This %(name)s instance is not fitted \"\n\u001b[1;32m--> 263\u001b[1;33m                                  \"yet\" % {'name': type(self).__name__})\n\u001b[0m\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFittedError\u001b[0m: This LogisticRegression instance is not fitted yet"
     ]
    }
   ],
   "source": [
    "prediction_dev = model.predict(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T04:56:07.884692Z",
     "start_time": "2019-05-25T04:56:07.879708Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Brisbane' 'Brisbane' 'Brisbane' ... 'Brisbane' 'Brisbane' 'Brisbane']\n"
     ]
    }
   ],
   "source": [
    "print(prediction_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T04:57:35.027670Z",
     "start_time": "2019-05-25T04:57:34.960852Z"
    }
   },
   "outputs": [],
   "source": [
    "summary_matrix = confusion_matrix_generator(prediction_dev, y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T04:57:42.521633Z",
     "start_time": "2019-05-25T04:57:42.514656Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Mel': {'Mel': 246, 'Syd': 183, 'Per': 134, 'Bri': 8766},\n",
       " 'Syd': {'Mel': 185, 'Syd': 186, 'Per': 131, 'Bri': 8827},\n",
       " 'Per': {'Mel': 240, 'Syd': 165, 'Per': 162, 'Bri': 8762},\n",
       " 'Bri': {'Mel': 202, 'Syd': 176, 'Per': 146, 'Bri': 8805}}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-23T05:35:34.610253Z",
     "start_time": "2019-05-23T05:35:34.572353Z"
    }
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-1bc0b90542ae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtitle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitles\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_dev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_dev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m#     acc = np.mean(cross_val_score(model, X_train, y_train, cv=10))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\site-packages\\sklearn\\dummy.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse_output_\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0matleast_1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_2d_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\site-packages\\numpy\\core\\shape_base.py\u001b[0m in \u001b[0;36matleast_1d\u001b[1;34m(*arys)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mary\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marys\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m         \u001b[0mary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36masanyarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    589\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m     \"\"\"\n\u001b[1;32m--> 591\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    592\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# read csv file\n",
    "# i = 1\n",
    "# train_X, train_y = load_dataset(train_filepath[i])\n",
    "# X_dev_no_0, y_dev_no_0 = remove_all_zero_instance(X_dev, y_dev)\n",
    "# dev_X, dev_y = load_dataset(dev_filepath[i])\n",
    "\n",
    "\n",
    "# try each model without feature selection\n",
    "for title, model in zip(titles, models):\n",
    "    start = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    acc = np.mean(cross_val_score(model, X_dev, y_dev, cv=10))\n",
    "#     acc = np.mean(cross_val_score(model, X_train, y_train, cv=10))\n",
    "    end = time.time()\n",
    "    t = end - start\n",
    "    print(title, acc, 'time:', t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T14:39:35.263814Z",
     "start_time": "2019-05-21T14:39:35.231902Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "class StackingClassifier():\n",
    "\n",
    "    def __init__(self, classifiers, metaclassifier):\n",
    "        self.classifiers = classifiers\n",
    "        self.metaclassifier = metaclassifier\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        for clf in self.classifiers:\n",
    "            clf.fit(X, y)\n",
    "        X_meta = self._predict_base(X)\n",
    "        self.metaclassifier.fit(X_meta, y)\n",
    "    \n",
    "    def _predict_base(self, X):\n",
    "        yhats = []\n",
    "        for clf in self.classifiers:\n",
    "            yhat = clf.predict_proba(X)\n",
    "            yhats.append(yhat)\n",
    "        yhats = np.concatenate(yhats, axis=1)\n",
    "        #print(yhats.shape)\n",
    "        assert len(yhats) == len(X)\n",
    "        return yhats\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_meta = self._predict_base(X)\n",
    "        yhat = self.metaclassifier.predict(X_meta)\n",
    "        return yhat\n",
    "    def score(self, X, y):\n",
    "        yhat = self.predict(X)\n",
    "        return accuracy_score(y, yhat)\n",
    "    \n",
    "\n",
    "\n",
    "classifiers = [LogisticRegression(),\n",
    "                KNeighborsClassifier(),\n",
    "                GaussianNB(),\n",
    "                MultinomialNB(),\n",
    "                DecisionTreeClassifier()]\n",
    "              \n",
    "meta_classifier = DecisionTreeClassifier()\n",
    "stacker = StackingClassifier(classifiers, meta_classifier)\n",
    "\n",
    "def load_car_data(car_file):\n",
    "    X = []\n",
    "    y = []\n",
    "    with open(car_file, mode='r') as fin:\n",
    "        for line in fin:\n",
    "            atts = line.strip().split(\",\")\n",
    "            X.append(atts[:-1]) #all atts minus the last one\n",
    "            y.append(atts[-1])\n",
    "    onehot = OneHotEncoder()\n",
    "    X = onehot.fit_transform(X).toarray()\n",
    "    return X, y\n",
    "# X, y = load_car_data('car.data')\n",
    "# print('labels:', set(y))\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "# stacker.fit(X_train, y_train)\n",
    "# print('stacker acc:', stacker.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T20:05:31.251280Z",
     "start_time": "2019-05-21T14:39:39.245739Z"
    }
   },
   "outputs": [],
   "source": [
    "stacker.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T13:14:42.409372Z",
     "start_time": "2019-05-21T13:13:54.696762Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stacker acc on train: 0.3938895553577648\n"
     ]
    }
   ],
   "source": [
    "print('stacker acc on train:', stacker.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T14:12:45.167657Z",
     "start_time": "2019-05-21T13:14:42.422337Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stacker cross-val acc  on train: 0.37537234507695616\n"
     ]
    }
   ],
   "source": [
    "cv = 5\n",
    "print('stacker cross-val acc  on train:', np.mean(cross_val_score(stacker.metaclassifier, X_train, y_train, cv=cv)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T14:12:51.612820Z",
     "start_time": "2019-05-21T14:12:45.172645Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stacker acc on dev: 0.31570907921534996\n"
     ]
    }
   ],
   "source": [
    "print('stacker acc on dev:', stacker.score(X_dev,y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T13:07:12.300370Z",
     "start_time": "2019-05-21T13:07:12.280422Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='cosine',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=3, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = KNeighborsClassifier(n_neighbors=3, algorithm='auto', metric = \"cosine\")\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T13:07:21.805816Z",
     "start_time": "2019-05-21T13:07:21.525565Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2890342420861846"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cross_val_score(model, X_dev, y_dev, cv=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T04:53:56.834364Z",
     "start_time": "2019-05-25T04:53:54.262241Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91260\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\91260\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model =  LogisticRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T09:53:22.920988Z",
     "start_time": "2019-05-21T09:52:58.787901Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T09:53:28.846271Z",
     "start_time": "2019-05-21T09:53:22.953900Z"
    }
   },
   "outputs": [],
   "source": [
    "predict_labels = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-21T14:39:45.091Z"
    }
   },
   "outputs": [],
   "source": [
    "predict_labels = stacker.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-21T14:39:46.041Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_prediction_to_submit_file(test_lables, filepath):\n",
    "    with open(filepath, 'w',  newline='') as writeFile:\n",
    "        writer = csv.writer(writeFile)\n",
    "        writer.writerows([[\"Id\", \"Class\"]])\n",
    "        for i in list(range(len(test_lables))):\n",
    "            index = '3' + str(i+1)\n",
    "            writer.writerows([[index, test_lables[i]]])\n",
    "        writeFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-21T14:39:47.496Z"
    }
   },
   "outputs": [],
   "source": [
    "test_prediction_to_submit_file(predict_labels, \"predicted_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "47.2px",
    "left": "42px",
    "top": "53.6px",
    "width": "164.113px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
